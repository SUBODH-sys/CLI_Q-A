{"text": "Question: How can I check if a program exists from a Bash script?\nAnswer: if ! command -v >/dev/null 2>&1 then echo \" could not be found\" exit 1 fi"}
{"text": "Question: How do I force \"git pull\" to overwrite local files? How do I force an overwrite of local files on a git pull?\nAnswer: First, update all origin/ refs to latest: git fetch --all Backup your current branch (e.g. main): git branch backup-main Jump to the latest commit on origin/main and checkout those files: git reset --hard origin/main"}
{"text": "Question: How can I count all the lines of code in a directory recursively? We've got a PHP application and want to count all the lines of code under a specific directory and its subdirectories.\nAnswer: Try: find . -name '*.php' | xargs wc -l or (when file names include special characters such as spaces) find . -name '*.php' | sed 's/.*/\"&\"/' | xargs wc -l The SLOCCount tool may help as well."}
{"text": "Question: Excluding directory when creating a .tar.gz file I have a /public_html/ folder, in that folder there's a /tmp/ folder that has like 70gb of files I don't really need. Now I am trying to create a .tar.gz of /public_html/ excluding /tmp/ This is the command I ran: tar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp/\" The tar is still being created, and by doing an ls -sh I can see that MyBackup.tar.gz already has about 30gb, and I know for sure that /public_html/ without /tmp/ doesn't have more than 1GB of files. What did I do wrong?\nAnswer: Try removing the last / at the end of the directory path to exclude tar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\""}
{"text": "Question: Tar archiving that takes input from a list of files I have a file that contain list of files I want to archive with tar. Let's call it mylist.txt It contains: file1.txt file2.txt ... file10.txt Is there a way I can issue TAR command that takes mylist.txt as input? Something like tar -cvf allfiles.tar -[someoption?] mylist.txt So that it is similar as if I issue this command: tar -cvf allfiles.tar file1.txt file2.txt file10.txt\nAnswer: Yes: tar -cvf allfiles.tar -T mylist.txt"}
{"text": "Question: How to escape single quotes within single quoted strings\nAnswer: If you really want to use single quotes in the outermost layer, you can use both kinds of quotation. Example: alias rxvt='urxvt -fg '\"'\"'#111111'\"'\"' -bg '\"'\"'#111111'\"'\" # ^^^^^ ^^^^^ ^^^^^ ^^^^ # 12345 12345 12345 1234 Explanation of how '\"'\"' is interpreted as just ': ' End first quotation which uses single quotes. \" Start second quotation, using double-quotes. ' Quoted character. \" End second quotation, using double-quotes. ' Start third quotation, using single quotes. If you do not place any whitespaces between (1) and (2), or between (4) and (5), the shell will interpret that string as a one long word: $ echo 'abc''123' abc123 $ echo 'abc'\\''123' abc'123 $ echo 'abc'\"'\"'123' abc'123 It will also keep the internal representation with 'to be joined' strings, and will also prefer the shorter escape syntax when possible: $ alias test='echo '\"'\"'hi'\"'\" $ alias test alias test='echo '\\''hi'\\''' $ test hi"}
{"text": "Question: Propagate all arguments in a Bash shell script I am writing a very simple script that calls another script, and I need to propagate the parameters from my current script to the script I am executing. For instance, my script name is foo.sh and calls bar.sh. foo.sh: bar $1 $2 $3 $4 How can I do this without explicitly specifying each parameter?\nAnswer: Use \"$@\" instead of plain $@ if you actually wish your parameters to be passed the same. Observe: $ cat no_quotes.sh #!/bin/bash ./echo_args.sh $@ $ cat quotes.sh #!/bin/bash ./echo_args.sh \"$@\" $ cat echo_args.sh #!/bin/bash echo Received: $1 echo Received: $2 echo Received: $3 echo Received: $4 $ ./no_quotes.sh first second Received: first Received: second Received: Received: $ ./no_quotes.sh \"one quoted arg\" Received: one Received: quoted Received: arg Received: $ ./quotes.sh first second Received: first Received: second Received: Received: $ ./quotes.sh \"one quoted arg\" Received: one quoted arg Received: Received: Received:"}
{"text": "Question: How do I get the directory where a Bash script is located from within the script itself? How do I get the path of the directory in which a Bash script is located, inside that script?\nAnswer: #!/usr/bin/env bash SCRIPT_DIR=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd ) is a useful one-liner which will give you the full directory name of the script no matter where it is being called from."}
{"text": "Question: How can I use grep to find a word inside a folder? In Windows, I would have done a search for finding a word inside a folder. Similarly, I want to know if a specific word occurs inside a directory containing many sub-directories and files. My searches for grep syntax shows I must specify the filename, i.e. grep string filename. Now, I do not know the filename, so what do I do? A friend suggested to do grep -nr string, but I don't know what this means and I got no results with it (there is no response until I issue a Ctrl + C).\nAnswer: grep -nr 'yourString*' . The dot at the end searches the current directory. Meaning for each parameter: -n Show relative line number in the file 'yourString*' String for search, followed by a wildcard character -r Recursively search subdirectories listed . Directory for search (current directory) grep -nr 'MobileAppSer*' . (Would find MobileAppServlet.java or MobileAppServlet.class or MobileAppServlet.txt; 'MobileAppASer*.*' is another way to do the same thing.) To check more parameters use man grep command."}
{"text": "Question: How to 'grep' a continuous stream? Is that possible to use grep on a continuous stream? What I mean is sort of a tail -f command, but with grep on the output in order to keep only the lines that interest me. I've tried tail -f | grep pattern but it seems that grep can only be executed once tail finishes, that is to say never.\nAnswer: Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.) tail -f file | grep --line-buffered my_pattern It looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below)."}
{"text": "Question: How to uncompress a tar.gz in another directory I have an archive *.tar.gz How can I uncompress this in a destination directory?\nAnswer: gzip -dc archive.tar.gz | tar -xf - -C /destination or, with GNU tar tar xzf archive.tar.gz -C /destination"}
{"text": "Question: Why is python saying I have \"no module named venv\"? I installed virtual env with sudo pip install virtualenv but when I run python -m venv flask I'm still getting this: /usr/bin/python: No module named venv Versions, if that's relevant: pip 1.5.6 from /usr/lib/python2.7/dist-packages (python 2.7) Python 2.7.9 what am I missing here?\nAnswer: venv is a module introduced in python3 venv is New in version 3.3."}
{"text": "Question: Replace one substring for another string in shell script?\nAnswer: To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}: #!/bin/bash firstString=\"I love Suzi and Marry\" secondString=\"Sara\" echo \"${firstString/Suzi/\"$secondString\"}\" # prints 'I love Sara and Marry' To replace all occurrences, use ${parameter//pattern/string}: message='The secret code is 12345' echo \"${message//[0-9]/X}\" # prints 'The secret code is XXXXX'"}
{"text": "Question: How do I split a string on a delimiter in Bash? I have this string stored in a variable: IN=\"bla@some.com;john@home.com\" ?\nAnswer: Set the internal field separator (IFS) variable, and then let it parse into an array."}
{"text": "Question: python subprocess doesn't inherit virtual environment When operating with a venv on Windoes 10 if I invoke a subprocess from a file in the directory, the subprocess does not seem to have access to the venv. Is there a way to make it work? Ideally I would like the approach to be portable to Linux but I'll take whatever gets the project running. Here is my test: main.py uses Popen to invoke sub_proc.py. sub_proc.py imports uuid_shortener, which has been installed in the virtual environment. If I run sub_proc.py directly it runs without an error. However, if I run main.py I see an error on the import statement for uuid_shortener. main.py import subprocess import time print(subprocess.Popen(['python', 'sub_proc.py'])) time.sleep(1) sub_proc.py import uuid_shortener Here is the output from running the code. (venv) PS C:\\Users\\...\\popenvenv> python .\\sub_proc.py (no error above) (venv) PS C:\\Users\\...\\popenvenv> python .\\main.py Traceback (most recent call last): File \"C:\\Users\\...\\popenvenv\\sub_proc.py\", line 1, in import uuid_shortener ModuleNotFoundError: No module named 'uuid_shortener' (venv) PS C:\\Users\\...\\popenvenv>\nAnswer: Use sys.executable in place of 'python'. sys.executable refers to the executable you're running with. This will preserve access to the virtualenv in subprocesses."}
{"text": "Question: How do I stash only one file out of multiple files that have changed?\nAnswer: git stash push -p -m"}
{"text": "Question: Looping through the content of a file in Bash How do I iterate through each line of a text file with Bash?\nAnswer: while read p; do echo \"$p\" done <peptides.txt"}
{"text": "Question: No module named pip in venv but pip installed I work in WSL Ubuntu. After instalation python3.13 dependencies from my previous projects stopped working. Venv with python 3.12 stopped activate in vscode interface. ErrorMessage: An Invalid Python interpreter is selected, please try changing it to enable features such as IntelliSense, linting, and debugging. See output for more details regarding why the interpreter is invalid. command \"source venv/bin/activate\" works but all libraries \"could not be resolvedPylance\". When I try reinstal I see error: ModuleNotFoundError: No module named 'pip' But pip installed in this venv. I can see it folders. init and main files etc. I have pip for python3.12 and I have -m venv for python 3.12. I can recreate this venv but why I can`t turn it on properly? Tried reinstall venv and pip for python3.12. python3 -m ensurepip is ok: Requirement already satisfied: pip in /usr/lib/python3/dist-packages (24.0) But python3.12 -m ensurepip ensurepip is disabled in Debian/Ubuntu for the system python. python3.12 -m pip --version is working also as python3.12 -m pip install and python3.12 -m pip --upgrade pip\nAnswer: I ran into the exact same problem after installing Python 3.13 on WSL. Suddenly, all my existing virtual environments (created with Python 3.12) broke in VSCode. I was getting the \"Invalid Python interpreter\" error, Pylance couldn't resolve any imports, and pip appeared to be missing\u2014even though I could see it in the venv/bin folder. Here\u2019s what fixed it for me: First, check what your system python3 now points to: python3 --version which python3 In my case, it was now Python 3.13, which explains why stuff started breaking. Your virtual environment still points to the Python 3.12 binary internally, but VSCode (and maybe even pip) is trying to use 3.13 instead. You can confirm that by looking at the pyvenv.cfg file inside your venv: cat venv/pyvenv.cfg You should see something like: home = /usr/bin/python3.12 If that's the case, then you just need to tell VSCode to use that exact interpreter. Open the command palette (Ctrl+Shift+P) in VSCode, choose \u201cPython: Select Interpreter\u201d, and manually select the path to your virtualenv\u2019s Python binary: /path/to/your/venv/bin/python Also, double-check the shebang in your pip script: head -n 1 venv/bin/pip If it says #!/usr/bin/python3, that might now point to Python 3.13, which breaks the venv. You can fix this by rebuilding the venv with the correct Python version: python3.12 -m venv --upgrade-deps venv Or, if that doesn\u2019t work cleanly: rm -rf venv python3.12 -m venv venv source venv/bin/activate pip install -r requirements.txt And yeah, ensurepip being disabled for system Python is normal on Ubuntu. Just make sure you have the necessary packages installed: sudo apt install python3.12-venv python3.12-distutils Once I manually selected the right interpreter in VSCode and fixed the pip shebang, everything worked again\u2014IntelliSense, linting, imports, etc. Hope that helps."}
{"text": "Question: How to add requirements.txt to uv environment I am working with uv for the first time and have created a venv to manage my dependencies. Now, I'd like to install some dependencies from a requirements.txt file. How can this be achieved with uv? I already tried manually installing each requirement using uv pip install .... However, this gets tedious for a large list of requirements.\nAnswer: You can install the dependencies to the virtual environment managed by uv using: uv pip install -r requirements.txt When working with a project (application or library) managed by uv, the following command might be used instead: uv add -r requirements.txt This will also add the requirements to the project's pyproject.toml."}
{"text": "Question: How do I change the URI (URL) for a remote Git repository?\nAnswer: First, view the existing remotes to verify which URL is currently set: git remote -v Then, you can set it with: git remote set-url origin See git help remote. You also can edit .git/config and change the URLs there."}
{"text": "Question: Is there a way to redirect output to a file and have it display on stdout?\nAnswer: tee: foo | tee output.file"}
{"text": "Question: Conda env vs venv / pyenv / virtualenv / Poetry / Docker, etc To add a question to the great question and discussion here on pyenv, venv, virtualenv, and virtualenvwrapper, could someone please explain how Conda environments fit into this world? When are the preferred use cases for Conda environments vs the other virtual environment options? Update to question in November 2024: how do Poetry and Docker fit into this world?\nAnswer: Here is how everything fits together. We'll start with the problems the various tools are solving. Then cover how each tool slots in. Dependency Managers. Problem these tools solve: What packages (dependencies) does the project need? For example: Django4.2, PyTest, Psycop2, etc. Can I install all those packages with one click? Do all those packages work together? e.g. the version of django I install works with the version of psycop, etc. Dependency/Package Managers solve the above questions for us. Environment Managers. Problems these solve: How do you have multiple projects that each need different things? For example, one project needs Python3.8 and Django4.2, but another needs Python3.12 and Django5.0? Environment managers create and manage \"virtual environments\", which allow each project to have its own set of packages/python versions installed. How do all the tools fit in? Some confusion is often caused by overlap in the tooling. Here is how they map: pip. Dependency management only. venv. Environment management only. virtualenv. Environment management only. Pipenv. Environment & Dependency management. Poetry. Environment & Dependency management. Conda. Environment & Dependency management. Conda is not just for python, but for R, Ruby, Node.js, etc. How does Docker fit in? Docker provides environment and dependency management, and also provides operating system management. For example, a Docker container can allow one project to operate on a Fedora Linux distribution, and another project to use Ubuntu. Docker can also be mixed and matched with the above tools, e.g. Docker handles the operating system management, but calls Poetry to handle environment and dependency management. Docker can also manage other dependencies too, like network settings, system libraries, etc. When to Use What? Development phase. Non-production. You want an environment manager and a dependency manager. It doesn't really matter which sets you use. venv/pip is fine. conda is fine. poetry is fine. Use whatever the people you work with use, or what you find easiest. There are some small differences between tools -- conda is a little slower to incorporate new python versions but has solid dependency compatibility checks, pip and env come with python and don't need to be installed separately, etc. -- but for the most part whatever you are most comfortable with or find easiest is what you should do unless you need a very specific feature offered by only of the tools. Deployment and production. With some exceptions you're probably going to want/need Docker. You're almost certainly going to need to use whatever tooling the rest of the team uses. Hope this helps everyone save a bunch of time understanding how all these tools fit together."}
{"text": "Question: How do I set a variable to the output of a command in Bash?\nAnswer: In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting. OUTPUT=\"$(ls -1)\" echo \"${OUTPUT}\" MULTILINE=\"$(ls \\ -1)\" echo \"${MULTILINE}\" Quoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised."}
{"text": "Question: How can I make grep print the lines below and above each matching line? I want to search each line for the word FAILED, then print the line above and below each matching line, as well as the matching line. Input: id : 15 Status : SUCCESS Message : no problem id : 15 Status : FAILED Message : connection error Desired output for grep 'FAILED': id : 15 Status : FAILED Message : connection error\nAnswer: grep's -A 1 option will give you one line after; -B 1 will give you one line before; and -C 1 combines both to give you one line both before and after, -1 does the same."}
{"text": "Question: How to extract tar file in Mac terminal As titled. I want to use some command, like for .zip files I can say unzip myfiles.zip -d mydirectory But is there a thing for .tar file on Mac as well?\nAnswer: Yes, you can run: tar -xvf myfile.tar For .tar.gz, you can run: tar -xzvf myfile.tar.gz If you want to extract to any directory other than your cwd, use -C. e.g: tar -xvf myfile.tar -C somedirectory I suggest you read the man page for tar if you wish to do anything further: man tar"}
{"text": "Question: python venv install skips component file \"pointer.png\" This is a strange issue. I maintain the pi3d python module and it contains this file github.com/tipam/pi3d/blob/master/src/pi3d/util/icons/pointer.png When I clone the repo locally it has the .png file but when the package is installed using pip it seems to be missing. This didn't used to be a problem. Is it something to do with the fact that pip insists on installing to a venv now, i.e. if I made pip install with --no-warn-script-location would it include the missing file?\nAnswer: It's because it's not present in tool.setuptools.package-data in pyproject.toml file. [tool.setuptools.package-data] \"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\"] With the previous configuration, you add all this extensions in your package as you can see in the next screenshot (content of the package uploaded on pypi). So adding the png extension should work: [tool.setuptools.package-data] \"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\", \"*.png\"]"}
{"text": "Question: Check the total content size of a tar gz file How can I extract the size of the total uncompressed file data in a .tar.gz file from command line?\nAnswer: This will sum the total content size of the extracted files: $ tar tzvf archive.tar.gz | sed 's/ \\+/ /g' | cut -f3 -d' ' | sed '2,$s/^/+ /' | paste -sd' ' | bc The output is given in bytes. Explanation: tar tzvf lists the files in the archive in verbose format like ls -l. sed and cut isolate the file size field. The second sed puts a + in front of every size except the first and paste concatenates them, giving a sum expression that is then evaluated by bc. Note that this doesn't include metadata, so the disk space taken up by the files when you extract them is going to be larger - potentially many times larger if you have a lot of very small files."}
{"text": "Question: How to get the process ID to kill a nohup process? I'm running a nohup process on the server. When I try to kill it my putty console closes instead. this is how I try to find the process ID: ps -ef |grep nohup this is the command to kill kill -9 1787 787\nAnswer: When using nohup and you put the task in the background, the background operator (&) will give you the PID at the command prompt. If your plan is to manually manage the process, you can save that PID and use it later to kill the process if needed, via kill PID or kill -9 PID (if you need to force kill). Alternatively, you can find the PID later on by ps -ef | grep \"command name\" and locate the PID from there. Note that nohup keyword/command itself does not appear in the ps output for the command in question. If you use a script, you could do something like this in the script: nohup my_command > my.log 2>&1 & echo $! > save_pid.txt This will run my_command saving all output into my.log (in a script, $! represents the PID of the last process executed). The 2 is the file descriptor for standard error (stderr) and 2>&1 tells the shell to route standard error output to the standard output (file descriptor 1). It requires &1 so that the shell knows it's a file descriptor in that context instead of just a file named 1. The 2>&1 is needed to capture any error messages that normally are written to standard error into our my.log file (which is coming from standard output). See I/O Redirection for more details on handling I/O redirection with the shell. If the command sends output on a regular basis, you can check the output occasionally with tail my.log, or if you want to follow it \"live\" you can use tail -f my.log. Finally, if you need to kill the process, you can do it via: kill -9 `cat save_pid.txt` rm save_pid.txt"}
{"text": "Question: How to create tar.gz archive file in Windows? How to create tar.gz archive of my files in Windows to upload and extract in cPanel?\nAnswer: tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows. If you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip."}
{"text": "Question: How to invert a grep expression The following grep expression successfully lists all the .exe and .html files in the current directory and sub directories. ls -R |grep -E .*[\\.exe]$\\|.*[\\.html]$ How do I invert this result to list those that aren't a .html or .exe instead. (That is, !=.)\nAnswer: Use command-line option -v or --invert-match, ls -R |grep -v -E .*[\\.exe]$\\|.*[\\.html]$"}
{"text": "Question: I want the date in YYYY-MM-DD format. How do I get this?\nAnswer: use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date)."}
{"text": "Question: How do I undo the most recent local commits in Git? I accidentally committed the wrong files to Git but haven't pushed the commit to the server yet. How do I undo those commits from the local repository?\nAnswer: $ git commit -m ; $ git reset HEAD~ ; $ git add; $ git commit -c ORIG_HEAD"}
{"text": "Question: Anaconda environment bash prefix too long I created an anaconda environment in a project folder specifying the path with -p option, i.e. not in the default anaconda3/envs folder: conda create -p venv The problem is that when I activate that environment, the bash prefix in the terminal is too long, i.e. it prepends the entire path of the environment to the prompt: (/path/to/the/environment/venv) user@machine: ~/path/to/environment/$ Is there a way to fix this, meaning make it shorter or delete the prefix from prompt? My $PS1: \\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$\nAnswer: Conda Prompt Customization Since Conda v4.6.0 there has been the env_prompt configuration option to provide for customization of the PS1 change. Here is the description: $ conda config --describe env_prompt # # env_prompt (str) # # Template for prompt modification based on the active environment. # # Currently supported template variables are '{prefix}', '{name}', and # # '{default_env}'. '{prefix}' is the absolute path to the active # # environment. '{name}' is the basename of the active environment # # prefix. '{default_env}' holds the value of '{name}' if the active # # environment is a conda named environment ('-n' flag), or otherwise # # holds the value of '{prefix}'. Templating uses python's str.format() # # method. # # # env_prompt: '({default_env}) ' One option that would help with your case would be to just use the {name} variable conda config --set env_prompt '({name}) ' which will show only the env's folder name. E.g., your example would be (venv) user@machine: ~/path/to/environment/$ Note, this will make it so that when the base env is active the prompt will show (anaconda3) instead of (base); otherwise, the other named envs should appear as usual. If you really can't stand that, you could run basename {default_env} to get the same result as {name} on unnamed envs and still retain base. That is, conda config --set env_prompt '(\\$(basename {default_env})) '"}
{"text": "Question: How do I squash my last N commits together into one commit?\nAnswer: Use git rebase -i and replace \"pick\" on the second and subsequent commits with \"squash\" or \"fixup\""}
{"text": "Question: How can I grep recursively, but only in files with certain extensions? I'm working on a script to grep certain directories: { grep -r -i CP_Image ~/path1/; grep -r -i CP_Image ~/path2/; grep -r -i CP_Image ~/path3/; grep -r -i CP_Image ~/path4/; grep -r -i CP_Image ~/path5/; } | mailx -s GREP email@domain.example How can I limit results only to extensions .h and .cpp?\nAnswer: Just use the --include parameter, like this: grep -inr --include \\*.h --include \\*.cpp CP_Image ~/path[12345] | mailx -s GREP email@domain.example That should do what you want. To take the explanation from HoldOffHunger's answer below: grep: command -r: recursively -i: ignore-case -n: each output line is preceded by its relative line number in the file --include \\*.cpp: all *.cpp: C++ files (escape with \\ just in case you have a directory with asterisks in the filenames) ./: Start at current directory."}
{"text": "Question: Native .tar extraction in Powershell I have a .tar.gz file that I need to extract. I've handled the gunzip bit with the GzipStream object from System.IO.Compression, but I couldn't find anything for dealing with tarballs in that namespace. Is there a way to deal with .tar files natively in Powershell? Note that it's only important that I be able to call any such function/method/object construction/system binary from a Powershell script; it doesn't need to actually be written in powershell. (If it matters I'm using 64-bit windows 10) P.S. please don't say \"use 7zip\"; that's not native\nAnswer: So it's been eleven days since I asked this and the general consensus is: \"No, there are no native tools in a vanilla window install that can handle tar extraction 'for you'\". This answer comes from Matthias R. Jensen and TessellatingHeckler, who both declined to answer outside of comments (I suspect due to not wanting to say \"no\" without an intimate knowledge of the entire Windows system architecture, which is fair). There are certainly scripts and classes and programs you can install, but nothing \"native\"."}
{"text": "Question: Tarballing without Git metadata My source tree contains several directories which are using Git source control, and I need to tarball the whole tree excluding any references to the Git metadata or custom log files. I thought I'd have a go using a combination of find/egrep/xargs/tar, but somehow the tar file contains the .git directories and the *.log files. This is what I have: find -type f . | egrep -v '\\.git|\\.log' | xargs tar rvf ~/app.tar Can someone explain my misunderstanding here? Why is tar processing the files that find and egrep are filtering? I'm open to other techniques as well.\nAnswer: You will get a nasty surprise when the number of files increase to more than one xargs command: Then you will first make a tar file of the first files and then overwrite the same tar file with the rest of the files. GNU tar has the --exclude option which will solve this issue: tar cvf ~/app.tar --exclude .git --exclude \"*.log\" ."}
{"text": "Question: What is the purpose of pyvenv.cfg after the creation of a Python virtual environment? When I create a virtual environment in Python on Windows cmd, in the virtual environment folder, the following files appear: Include Lib Scripts pyvenv.cfg What is the goal of pyvenv.cfg creation? Can I use it in any way?\nAnswer: pyvenv.cfg is a configuration file that stores information about the virtual environment such as standard libraries path Python version interpreter version virtual env flags or any other venv configs If print content of pyvenv.cfg $ cat myenv/pyvenv.cfg home = /usr/bin include-system-site-packages = false version = 3.8.5"}
{"text": "Question: How can I upgrade pip inside a venv inside a Dockerfile? While running $ sudo docker build -t myproj:tag . I am hit with the message You are using pip version 10.0.1, however version 18.0 is available. You should consider upgrading via the 'pip install --upgrade pip' command. and given recent occasional subtleties manifesting themselves with the error: \"/usr/bin/pip\" \"from pip import main\" \"ImportError: cannot import ..\" I'd rather yield and indeed upgrade. And so I add the pip upgrade command in the DockerFile, after the venv is built, since the pip that matters is the one inside the venv (am I getting this right?). So my Dockerfile now has this: ... RUN python -m venv venv RUN pip install --upgrade pip ... But doing so does not avoid the \"You are using pip 10.x\" message. What am I missing? Update Though a promising suggestion, neither RUN source venv/bin/activate RUN pip install --upgrade pip nor RUN source venv/bin/activate RUN python -m pip install --upgrade pip eliminate the \"You are using pip version 10.0.1, ...\" message.\nAnswer: The single easiest answer to this is to just not bother with a virtual environment in a Docker image. A virtual environment gives you an isolated filesystem space with a private set of Python packages that don't conflict with the system install, but so does a Docker image. You can just use the system pip in a Docker image and it will be fine. FROM python:3.7 RUN pip install --upgrade pip WORKDIR /usr/src/app COPY . . RUN pip install . CMD [\"myscript\"] If you really want a virtual environment, you either need to specifically run the wrapper scripts from the virtual environment's path RUN python -m venv venv RUN venv/bin/pip install --upgrade pip or run the virtual environment \"activate\" script on every RUN command; the environment variables it sets won't carry over from one step to another. (Each RUN command in effect does its own docker run; docker commit sequence under the hood and will launch a new shell in a new container; the Dockerfile reference describes this a little bit.) RUN python -m venv venv RUN . venv/bin/activate \\ && pip install --upgrade pip COPY . . RUN . venv/bin/activate \\ && pip install . CMD [\"venv/bin/myscript\"] Trying to activate the virtual environment in its own RUN instruction does nothing beyond generate a no-op layer. # This step does nothing RUN . venv/bin/activate # And therefore this upgrades the system pip RUN pip install --upgrade pip"}
{"text": "Question: How to extract tar archive from stdin? I have a large tar file I split. Is it possible to cat and untar the file using pipeline. Something like: cat largefile.tgz.aa largefile.tgz.ab | tar -xz instead of: cat largefile.tgz.aa largfile.tgz.ab > largefile.tgz tar -xzf largefile.tgz I have been looking around and I can't find the answer. I wanted to see if it was possible.\nAnswer: Use - as the input file: cat largefile.tgz.aa largefile.tgz.ab | tar zxf - Make sure you cat them in the same order they were split. If you're using zsh you can use the multios feature and avoid invoking cat: < largefile.tgz.aa < largefile.tgz.ab tar zxf - Or if they are in alphabetical order: <largefile.tgz.* | tar zxf -"}
{"text": "Question: How do I check if a directory exists or not in a Bash shell script?\nAnswer: To check if a directory exists: if [ -d \"$DIRECTORY\" ]; then echo \"$DIRECTORY does exist.\" fi To check if a directory does not exist: if [ ! -d \"$DIRECTORY\" ]; then echo \"$DIRECTORY does not exist.\" fi"}
{"text": "Question: How can I grep Git commits for a certain word? In a Git code repository I want to list all commits that contain a certain word. I tried this git log -p | grep --context=4 \"word\" but it does not necessarily give me back the filename (unless it's less that five lines away from the word I searched for. I also tried git grep \"word\" but it gives me only present files and not the history. How do I search the entire history, so I can follow changes on a particular word? I intend to search my codebase for occurrences of word to track down changes (search in files history).\nAnswer: If you want to find all commits where the commit message contains a given word, use git log --grep=word If you want to find all commits where \"word\" was added or removed in the file contents (to be more exact: where the number of occurrences of \"word\" changed), i.e., search the commit contents, use a so-called 'pickaxe' search with git log -Sword In modern Git there is also git log -Gword to look for differences whose added or removed line matches \"word\" (also commit contents). A few things to note: -G by default accepts a regex, while -S accepts a string, but it can be modified to accept regexes using the --pickaxe-regex. -S finds commits where the number of occurrences of \"word\" changed, while -G finds commits where \"word\" appears in the diff. This means that -S --pickaxe-regex and -G do not do exactly the same thing. The git diff documentation has a nice explanation of the difference: To illustrate the difference between -S --pickaxe-regex and -G, consider a commit with the following diff in the same file: + return frotz(nitfol, two->ptr, 1, 0); ... - hit = frotz(nitfol, mf2.ptr, 1, 0); While git log -G\"frotz\\(nitfol\" will show this commit, git log -S\"frotz\\(nitfol\" --pickaxe-regex will not (because the number of occurrences of that string did not change). This will show the commits containing the search terms, but if you want to see the actual changes in those commits instead you can use --patch: git log -G\"searchTerm\" --patch This can then be piped to grep to isolate the output just to display commit diff lines with that search term. A common use-case is to display diff lines with that search term in commits since and including a given commit - 3b5ab0f2a1 in this example - like so: git log 3b5ab0f2a1^.. -G\"searchTerm\" --patch | grep searchTerm"}
{"text": "Question: What does \" 2>&1 \" mean? To combine stderr and stdout into the stdout stream, we append this to a command: 2>&1?\nAnswer: File descriptor 1 is the standard output (stdout). File descriptor 2 is the standard error (stderr). We use 2>&1. Consider >& to be a redirect merger operator."}
{"text": "Question: remove virtual environment created with venv in python3 How can I delete a virtual environement created with python3 -m venv Can I just remove the directory? This seems like a question googling should easily answer, but I only found answers for deleting environments created with virtualenv or pyvenv.\nAnswer: Yes, delete the directory. it's where executables for the venv and modules and libraries and entire other stuff for venvs is kept."}
{"text": "Question: Extract tar the tar.bz2 file error I tried to extract the tar.bz2 file in Fedora 17 OS. I used the command: # tar -xvjf myfile.tar.bz2 I received this error message: tar (child):bzip2: Cannot exec :Nosuch of file or directory tar (child): Error is not recoverable: exitng now tar: Child returned status 2 tar:Error is not recoverable: exitng now How can I resolve this?\nAnswer: For bz2 you need to execute like this, tar -jxvf Alternatively, you can also execute like this bunzip2 myfile.tar.bz2 For more information you should check it, tar --help If in doubt, run file on the archive to make sure it actually is compressed in bz2 format."}
{"text": "Question: How do I resolve merge conflicts in a Git repository?\nAnswer: Try: git mergetool It opens a GUI that steps you through each conflict, and you get to choose how to merge."}
{"text": "Question: How can I reset or revert a file to a specific revision?\nAnswer: Assuming the hash of the commit you want is c5f567: git checkout c5f567 -- file1/to/restore file2/to/restore"}
{"text": "Question: When working with a venv virtual environment, which files should I be commiting to my git repository? Using GitHub's .gitignore, I was able to filter out some files and directories. However, there's a few things that left me a little bit confused: GitHub's .gitignore did not include /bin and /share created by venv. I assumed they should be ignored by git, however, as the user is meant to build the virtual environment themselves. Pip generated a pip-selfcheck.json file, which seemed mostly like clutter. I assume it usually does this, and I just haven't seen the file before because it's been placed with my global pip. pyvenv.cfg is what I really can't make any sense of, though. On one hand, it specifies python version, which ought to be needed for others who want to use the project. On the other hand, it also specifies home = /usr/bin, which, while perhaps probably correct on a lot of Linux distributions, won't necessarily apply to all systems. Are there any other files/directories I missed? Are there any stricter guidelines for how to structure a project and what to include?\nAnswer: Although venv is a very useful tool, you should not assume (unless you have good reason to do so) that everyone who looks at your repository uses it. Avoid committing any files used only by venv; these are not strictly necessary to be able to run your code and they are confusing to people who don't use venv. The only configuration file you need to include in your repository is the requirements.txt file generated by pip freeze > requirements.txt which lists package dependencies. You can then add a note in your readme instructing users to install these dependencies with the command pip install -r requirements.txt. It would also be a good idea to specify the required version of Python in your readme."}
{"text": "Question: How do I update a Python virtual environment with `venv` (in Python 3.3+) to use a newer version of Python? I have recently installed Python 3.8.0 alongside Python 3.7.4. I have some virtual environments (created using python -m venv that are based on v3.7.4. How do I update them to use v3.8.0? Do I need to create a new virtual environment and reinstall the dependencies, scripts, etc.? Note: There are some existing Q&A's (such as this) that deal with the older virtualenv package/tool. I'm specifically asking about the new built-in venv module, which is a standard built-in to Python since v3.3 and has some differences from virtualenv.\nAnswer: I guess what you're looking for is the --upgrade parameter. python -m venv --help usage: venv [-h] [--system-site-packages] [--symlinks | --copies] [--clear] [--upgrade] [--without-pip] [--prompt PROMPT] ENV_DIR [ENV_DIR ...] Creates virtual Python environments in one or more target directories. positional arguments: ENV_DIR A directory to create the environment in. optional arguments: -h, --help show this help message and exit --system-site-packages Give the virtual environment access to the system site-packages dir. --symlinks Try to use symlinks rather than copies, when symlinks are not the default for the platform. --copies Try to use copies rather than symlinks, even when symlinks are the default for the platform. --clear Delete the contents of the environment directory if it already exists, before environment creation. --upgrade Upgrade the environment directory to use this version of Python, assuming Python has been upgraded in-place. --without-pip Skips installing or upgrading pip in the virtual environment (pip is bootstrapped by default) --prompt PROMPT Provides an alternative prompt prefix for this environment. You need to run it with the targeted python version, for example in this case: python3.8 -m venv --upgrade Assuming that python3.8 is the name of your python 3.8.0 executable."}
{"text": "Question: How do I extract a tar file in Java? How do I extract a tar (or tar.gz, or tar.bz2) file in Java?\nAnswer: Note: This functionality was later published through a separate project, Apache Commons Compress, as described in another answer. This answer is out of date. I haven't used a tar API directly, but tar and bzip2 are implemented in Ant; you could borrow their implementation, or possibly use Ant to do what you need. Gzip is part of Java SE (and I'm guessing the Ant implementation follows the same model). GZIPInputStream is just an InputStream decorator. You can wrap, for example, a FileInputStream in a GZIPInputStream and use it in the same way you'd use any InputStream: InputStream is = new GZIPInputStream(new FileInputStream(file)); (Note that the GZIPInputStream has its own, internal buffer, so wrapping the FileInputStream in a BufferedInputStream would probably decrease performance.)"}
{"text": "Question: How to Count number of lines in a non binary file (Like a CSV or a TXT) file in terminal\nAnswer: wc -l This will output the number of lines in textfile"}
{"text": "Question: Cannot import: `from serpapi import GoogleSearch` I have this app in PyCharm (see screenshot). It won't run because of: ImportError: cannot import name 'GoogleSearch' from 'serpapi' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/serpapi/init.py) Process finished with exit code 1 Has anybody faced this issue? Invalidate Caches / Restart pycharm. Check my run configuration\nAnswer: The solution was to remove all system Python installations from my Mac (including the official Python binary and Brew python). Then create a run shell script configuration in PyCharm like this: source venv/bin/activate && streamlit run app/web/app.py"}
{"text": "Question: Rename Directory Name Before tar Happens I have a directory e.g. /var/tmp/my-dir/ that I frequently compress with the following command: $ cd /var/tmp/ $ tar -zcf my-dir.tar.gz my-dir/* Later, when I untar my-dir.tar.gz, it'll create my-dir/ in the current directory. It sounds like the my-dir directory is \"wrapped\" inside the tarball. Is there a tar option to rename my-dir to e.g. your-dir before the actual tarring happens. So that ... $ tar -zxf my-dir.tar.gz # So that ... this creates your-dir/, instead of my-dir/ Thanks.\nAnswer: Which tar? GNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames. For example, to rename during unpacking: tar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/ BSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression)."}
{"text": "Question: Extract filename and extension in Bash I want to get the filename (without extension) and the extension separately.\nAnswer: First, get file name without the path: filename=$(basename -- \"$fullfile\") extension=\"${filename##*.}\" filename=\"${filename%.*}\" e file extensions: filename=\"${fullfile##*/}\""}
{"text": "Question: How to search contents of multiple pdf files? How could I search the contents of PDF files in a directory/subdirectory? I am looking for some command line tools. It seems that grep can't search PDF files.\nAnswer: Your distribution should provide a utility called pdftotext: find /path -name '*.pdf' -exec sh -c 'pdftotext \"{}\" - | grep --with-filename --label=\"{}\" --color \"your pattern\"' \\; The \"-\" is necessary to have pdftotext output to stdout, not to files. The --with-filename and --label= options will put the file name in the output of grep. The optional --color flag is nice and tells grep to output using colors on the terminal. (In Ubuntu, pdftotext is provided by the package xpdf-utils or poppler-utils.) This method, using pdftotext and grep, has an advantage over pdfgrep if you want to use features of GNU grep that pdfgrep doesn't support. Note: pdfgrep-1.3.x supports -C option for printing line of context."}
{"text": "Question: Tar a directory, but don't store full absolute paths in the archive I have the following command in the part of a backup shell script: tar -cjf site1.bz2 /var/www/site1/ When I list the contents of the archive, I get: tar -tf site1.bz2 var/www/site1/style.css var/www/site1/index.html var/www/site1/page2.html var/www/site1/page3.html var/www/site1/images/img1.png var/www/site1/images/img2.png var/www/site1/subdir/index.html But I would like to remove the part /var/www/site1 from directory and file names within the archive, in order to simplify extraction and avoid useless constant directory structure. Never know, in case I would extract backuped websites in a place where web data weren't stored under /var/www. For the example above, I would like to have : tar -tf site1.bz2 style.css index.html page2.html page3.html images/img1.png images/img2.png subdir/index.html So, that when I extract, files are extracted in the current directory and I don't need to move extracted files afterwards, and so that sub-directory structures is preserved. There are already many questions about tar and backuping in stackoverflow and at other places on the web, but most of them ask for dropping the entire sub-directory structure (flattening), or just add or remove the initial / in the names (I don't know what it changes exactly when extracting), but no more. After having read some of the solutions found here and there as well as the manual, I tried : tar -cjf site1.bz2 -C . /var/www/site1/ tar -cjf site1.bz2 -C / /var/www/site1/ tar -cjf site1.bz2 -C /var/www/site1/ /var/www/site1/ tar -cjf site1.bz2 --strip-components=3 /var/www/site1/ But none of them worked the way I want. Some do nothing, some others don't archive sub-directories anymore. It's inside a backup shell script launched by a Cron, so I don't know well, which user runs it, what is the path and the current directory, so always writing absolute path is required for everything, and would prefer not changing current directory to avoid breaking something further in the script (because it doesn't only backup websites, but also databases, then send all that to FTP etc.) How to achieve this? Have I just misunderstood how the option -C works?\nAnswer: tar -cjf site1.tar.bz2 -C /var/www/site1 . In the above example, tar will change to directory /var/www/site1 before doing its thing because the option -C /var/www/site1 was given. From man tar: OTHER OPTIONS -C, --directory DIR change to directory DIR"}
{"text": "Question: Check existence of input argument in a Bash shell script I need to check the existence of an input argument.\nAnswer: if [ $# -eq 0 ] then echo \"No arguments supplied\" fi The $# variable will tell you the number of input arguments the script was passed."}
{"text": "Question: Match two strings in one line with grep I am trying to use grep to match lines that contain two different strings. I have tried the following but this matches lines that contain either string1 or string2 which not what I want. grep 'string1\\|string2' filename So how do I match with grep only the lines that contain both strings?\nAnswer: You can use grep 'string1' filename | grep 'string2' This searches for string1 followed by string 2 on the same line, or string2 followed by string1 on the same line; it does not answer the question: grep 'string1.*string2\\|string2.*string1' filename"}
{"text": "Question: Delete all local Git branches I follow a development process where I create a new local branch for every new feature or story card. When finished I merge the branch into master and then push. What tends to happen over time due to a combination of laziness or forgetfulness, is that I end up with a large list of local branches, some of which (such as spikes) may not have been merged. I know how to list all my local branches and I know how to remove a single branch but I was wondering if there was a Git command that allows me to delete all my local branches? Below is the output of the git branch --merged command. cd ~/projects/application git branch --merged Output: STORY-123-Short-Description STORY-456-Another-Description STORY-789-Blah-Blah * master All attempts to delete branches listed with grep -v \\* (as per the answers below) result in errors: error: branch 'STORY-123-Short-Description' not found. error: branch 'STORY-456-Another-Description' not found. error: branch 'STORY-789-Blah-Blah' not found. I'm using: Git 1.7.4.1 Ubuntu 10.04 (Lucid Lynx) GNU Bash, version 4.1.5(1)-release GNU grep 2.5.4\nAnswer: The 'git branch -d' subcommand can delete more than one branch. So, simplifying @sblom's answer but adding a critical xargs: git branch -D `git branch --merged | grep -v \\* | xargs` or, further simplified to: git branch --merged | grep -v \\* | xargs git branch -D Importantly, as noted by @AndrewC, using git branch for scripting is discouraged. To avoid it use something like: git for-each-ref --format '%(refname:short)' refs/heads | grep -v \"master\\|main\" | xargs git branch -D Caution warranted on deletes! $ mkdir br $ cd br; git init Initialized empty Git repository in /Users/ebg/test/br/.git/ $ touch README; git add README; git commit -m 'First commit' [master (root-commit) 1d738b5] First commit 0 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 README $ git branch Story-123-a $ git branch Story-123-b $ git branch Story-123-c $ git branch --merged Story-123-a Story-123-b Story-123-c * master $ git branch --merged | grep -v \\* | xargs Story-123-a Story-123-b Story-123-c $ git branch --merged | grep -v \\* | xargs git branch -D Deleted branch Story-123-a (was 1d738b5). Deleted branch Story-123-b (was 1d738b5). Deleted branch Story-123-c (was 1d738b5)."}
{"text": "Question: How do I recursively grep all directories and subdirectories? How do I recursively grep all directories and subdirectories? find . | xargs grep \"texthere\" *\nAnswer: grep -r \"texthere\" . The first parameter represents the regular expression to search for, while the second one represents the directory that should be searched. In this case, . means the current directory. Note: This works for GNU grep, and on some platforms like Solaris you must specifically use GNU grep as opposed to legacy implementation. For Solaris this is the ggrep command."}
{"text": "Question: How do I untar a subdirectory into the current directory? How to I extract a subdirectory in a tarball into the current directory? Example, the tarball from wordpress: wordpress/ wordpress/wp-trackback.php wordpress/wp-config-sample.php wordpress/wp-settings.php wordpress/wp-rss2.php wordpress/readme.html wordpress/index.php ... How do I extract everything under wordpress/ into the current directory? In otherwords, it will not create a wordpress directory. I've tried this with no luck: tar xvfz latest.tar.gz wordpress -C ./ I know I can extract it normally and move it back, but I figure there has to be a way to do it in one shot.\nAnswer: Why don't you untar normally, then just: mv wordpress/.* . mv wordpress/* . rmdir wordpress But alas, there's: tar --strip-components=1 -zxvf wordpress.tgz"}
{"text": "Question: Find all files containing a specific text (string) on Linux? How do I find all files containing a specific string of text within their file contents? The following doesn't work. It seems to display every single file in the system. find / -type f -exec grep -H 'text-to-find-here' {} \\;\nAnswer: Do the following: grep -rnw '/path/to/somewhere/' -e 'pattern' -r or -R is recursive, -n is line number, and -w stands for match the whole word. -l (lower-case L) can be added to just give the file name of matching files. -e is the pattern used during the search Along with these, --exclude, --include, --exclude-dir flags could be used for efficient searching: This will only search through those files which have .c or .h extensions: grep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\" This will exclude searching all the files ending with .o extension: grep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\" For directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/: grep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\" This works very well for me, to achieve almost the same purpose like yours. For more options, see man grep."}
{"text": "Question: Highlight text similar to grep, but don't filter out text When using grep, it will highlight any text in a line with a match to your regular expression. What if I want this behaviour, but have grep print out all lines as well? I came up empty after a quick look through the grep man page.\nAnswer: Use ack. Check out its --passthru option here: ack. It has the added benefit of allowing full Perl regular expressions. $ ack --passthru 'pattern1' file_name $ command_here | ack --passthru 'pattern1' You can also do it using grep like this: $ grep --color -E '^|pattern1|pattern2' file_name $ command_here | grep --color -E '^|pattern1|pattern2' This will match all lines and highlight the patterns. The ^ matches every start of the line but won't get printed/highlighted since it's not a character. (Note that most of the setups will use --color by default. You may not need that flag)."}
{"text": "Question: How do I create a remote Git branch? I created a local branch. How do I push it to the remote server?\nAnswer: git push.default = current , then push a new branch upstream with $ git push -u -u will track remote branch of the same name."}
{"text": "Question: How do I add an empty directory to a Git repository?\nAnswer: create a .gitignore file inside that directory that contains these four lines: # Ignore everything in this directory * # Except this file !.gitignore."}
{"text": "Question: Display filename before matching line How can I get grep to display the filename before the matching lines in its output?\nAnswer: Try this little trick to coax grep into thinking it is dealing with multiple files, so that it displays the filename: grep 'pattern' file /dev/null To also get the line number: grep -n 'pattern' file /dev/null"}
{"text": "Question: How can I exclude directories from grep -R? I want to traverse all subdirectories, except the node_modules directory.\nAnswer: SOLUTION 1 (combine find and grep) The purpose of this solution is not to deal with grep performance but to show a portable solution : should also work with busybox or GNU version older than 2.5. Use find, for excluding directories foo and bar : find /dir \\( -name foo -prune \\) -o \\( -name bar -prune \\) -o -name \"*.sh\" -print Then combine find and the non-recursive use of grep, as a portable solution : find /dir \\( -name node_modules -prune \\) -o -name \"*.sh\" -exec grep --color -Hn \"your text to find\" {} 2>/dev/null \\; SOLUTION 2 (recursive use of grep, using the -R and --exclude-dir option of grep): You know this solution already (recursive search), but I add it since it's the most recent and efficient solution. Note this is a less portable solution but more human-readable. grep -R --exclude-dir=node_modules 'some pattern' /path/to/search To exclude multiple directories, use --exclude-dir as: --exclude-dir={node_modules,dir1,dir2,dir3} Note the \"-R\" option for a deference recursive file reading, i.e. following ALL sym links, unlike \"-r\". SOLUTION 3 (Ag) If you frequently search through code, Ag (The Silver Searcher) is a much faster alternative to grep, that's customized for searching code. For instance, it automatically ignores files and directories listed in .gitignore, so you don't have to keep passing the same cumbersome exclude options to grep or find."}
{"text": "Question: Invalid Python interpreter selected for the project I use Pycharm, and whenever I move a project to another folder, it says \"invalid python interpreter selected for the project\". I think it happens because the path to venv has changed. I tried Configure Python Interpreter > Add Interpreter > Select \"Existing Environment\" with the new path to venv. I was expecting this to work, but after I click OK, it says \"Updating interpreter path\" for a moment, and nothing changes. I tried even checking \"make available to all projects\". I also tried some of stack overflow answers, but couldn't make it. How can I fix this? Error:Cannot run program \"C:\\Users\\aksha\\Pycharm\\Digital Clock\\venv\\Scripts\\python.exe\" (in directory \"C:\\Users\\aksha\\Pycharm\\60 Python Projects\\Digital Clock\"): CreateProcess error=2, The system cannot find the file specified\nAnswer: Virtualenv has hardcoded path inside the bin/activate file, which will fail when you move the virtualenv. One way is to recreate your virtualenv. Another way is to open the file bin/activate and find this line VIRTUAL_ENV='/private/tmp/test2/.venv' and replace it with your new virtualenv path. My virtualenv is created at /tmp/test2/.venv for reference."}
{"text": "Question: How can I declare and use Boolean variables in a shell script?\nAnswer: For example: the_world_is_flat=true # ...do something interesting... if [ \"$the_world_is_flat\" = true ] ; then echo 'Be careful not to fall off!' fi"}
{"text": "Question: Are tar.gz and tgz the same thing? I created .tgz file with tar czvf file command.then I ended up with a tgz file. I want to know the difference between it and tar.gz.\nAnswer: I think in the old package repo days, .tgz was used because files on DOS floppies could only have three letter extensions. When this limitation was removed, .tar.gz was used to be more verbose by showing both the archive type (tar) and zipper (gzip). They are identical."}
{"text": "Question: Control the pip version in virtualenv How do I control the version of pip which is used in a freshly created venv? By default, it uses a vendored pip distribution which may be out of date or unsuitable for whatever other reason. I want to be able to create a venv with a user-specified version of pip installed initially, as opposed to creating one and then upgrading the pip installation from within the env.\nAnswer: Since Python 3.9 the stdlib venv module has EnvBuilder.upgrade_dependencies. Unfortunately, it has two shortcomings: Won't really help users to install a specific pip version, only the latest. It still installs the vendored pip and setuptools versions first, and then uninstall them if they're outdated, which they almost always will be in practice. It would be ideal to install the latest versions directly! The venv CLI provides a --without-pip argument that is useful here. You can use this to opt-out of the vendored pip, and then actually use the vendored pip wheel to install your desired pip version instead (along with any other packages you might want in a freshly created virtual environment). It's best to put it into a function - this goes into your shell profile or rc file: function ve() { local py=\"python3\" if [ ! -d ./.venv ]; then echo \"creating venv...\" if ! $py -m venv .venv --prompt=$(basename $PWD) --without-pip; then echo \"ERROR: Problem creating venv\" >&2 return 1 else local whl=$($py -c \"import pathlib, ensurepip; [whl] = pathlib.Path(ensurepip.__path__[0]).glob('_bundled/pip*.whl'); print(whl)\") echo \"boostrapping pip using $whl\" .venv/bin/python $whl/pip install --upgrade pip setuptools wheel source .venv/bin/activate fi else source .venv/bin/activate fi } As written, this function just pulls latest pip, setuptools, and wheel from index. To force specific versions you can just change this line of the shell script: .venv/bin/python $whl/pip install --upgrade pip setuptools wheel Into this, for example: .venv/bin/python $whl/pip install pip==19.3.1 For Python 2.7 users, you may do a similar trick because virtualenv provides similar command-line options in --no-pip, --no-setuptools, and --no-wheel, and there is still a vendored pip wheel available to bootstrap since Python 2.7.9. Pathlib will not be available, so you'll need to change the pathlib usage into os.path + glob."}
{"text": "Question: How do I grep for all non-ASCII characters? I have several very large XML files and I'm trying to find the lines that contain non-ASCII characters. I've tried the following: grep -e \"[\\x{00FF}-\\x{FFFF}]\" file.xml But this returns every line in the file, regardless of whether the line contains a character in the range specified. Do I have the syntax wrong or am I doing something else wrong? I've also tried: egrep \"[\\x{00FF}-\\x{FFFF}]\" file.xml (with both single and double quotes surrounding the pattern).\nAnswer: You can use the command: LC_ALL=C grep --color='auto' -P -n \"[\\x80-\\xFF]\" file.xml This will give you the line number, and will highlight non-ascii chars in red. In some systems, depending on your settings, the above will not work, so you can grep by the inverse LC_ALL=C grep --color='auto' -P -n \"[^\\x00-\\x7F]\" file.xml Note also, that the important bit is the -P flag which equates to --perl-regexp: so it will interpret your pattern as a Perl regular expression. It also says that this is highly experimental and grep -P may warn of unimplemented features."}
{"text": "Question: How do I remove/delete a virtualenv? I created an environment with the following command: virtualenv venv --distribute Trying to remove it with the following command: rmvirtualenv venv does not work. I do an lson my current directory and I still see venv The only way I can remove it seems to be: sudo rm -rf venv Note that the environment is not active. I'm running Ubuntu 11.10. Any ideas? I've tried rebooting my system to no avail.\nAnswer: \"The only way I can remove it seems to be: sudo rm -rf venv\" That's it! There is no command for deleting your virtual environment. Simply deactivate it and rid your application of its artifacts by recursively removing it. Note that this is the same regardless of what kind of virtual environment you are using. virtualenv, venv, Anaconda environment, pyenv, pipenv are all based the same principle here."}
{"text": "Question: Parsing JSON with Unix tools I'm trying to parse JSON returned from a curl request, like so: curl 'http://twitter.com/users/username.json' | sed -e 's/[{}]/''/g' | awk -v k=\"text\" '{n=split($0,a,\",\"); for (i=1; i<=n; i++) print a[i]}' The above splits the JSON into fields, for example: % ... \"geo_enabled\":false \"friends_count\":245 \"profile_text_color\":\"000000\" \"status\":\"in_reply_to_screen_name\":null \"source\":\"web\" \"truncated\":false \"text\":\"My status\" \"favorited\":false % ... How do I print a specific field (denoted by the -v k=text)?\nAnswer: There are a number of tools specifically designed for the purpose of manipulating JSON from the command line, and will be a lot easier and more reliable than doing it with Awk, such as jq: curl -s 'https://api.github.com/users/lambda' | jq -r '.name' You can also do this with tools that are likely already installed on your system, like Python using the json module, and so avoid any extra dependencies, while still having the benefit of a proper JSON parser. The following assume you want to use UTF-8, which the original JSON should be encoded in and is what most modern terminals use as well: Python 3: curl -s 'https://api.github.com/users/lambda' | \\ python3 -c \"import sys, json; print(json.load(sys.stdin)['name'])\" Python 2: export PYTHONIOENCODING=utf8 curl -s 'https://api.github.com/users/lambda' | \\ python2 -c \"import sys, json; print json.load(sys.stdin)['name']\" Frequently Asked Questions Why not a pure shell solution? The standard POSIX/Single Unix Specification shell is a very limited language which doesn't contain facilities for representing sequences (list or arrays) or associative arrays (also known as hash tables, maps, dicts, or objects in some other languages). This makes representing the result of parsing JSON somewhat tricky in portable shell scripts. There are somewhat hacky ways to do it, but many of them can break if keys or values contain certain special characters. Bash 4 and later, zsh, and ksh have support for arrays and associative arrays, but these shells are not universally available (macOS stopped updating Bash at Bash 3, due to a change from GPLv2 to GPLv3, while many Linux systems don't have zsh installed out of the box). It's possible that you could write a script that would work in either Bash 4 or zsh, one of which is available on most macOS, Linux, and BSD systems these days, but it would be tough to write a shebang line that worked for such a polyglot script. Finally, writing a full fledged JSON parser in shell would be a significant enough dependency that you might as well just use an existing dependency like jq or Python instead. It's not going to be a one-liner, or even small five-line snippet, to do a good implementation. Why not use awk, sed, or grep? It is possible to use these tools to do some quick extraction from JSON with a known shape and formatted in a known way, such as one key per line. There are several examples of suggestions for this in other answers. However, these tools are designed for line based or record based formats; they are not designed for recursive parsing of matched delimiters with possible escape characters. So these quick and dirty solutions using awk/sed/grep are likely to be fragile, and break if some aspect of the input format changes, such as collapsing whitespace, or adding additional levels of nesting to the JSON objects, or an escaped quote within a string. A solution that is robust enough to handle all JSON input without breaking will also be fairly large and complex, and so not too much different than adding another dependency on jq or Python. I have had to deal with large amounts of customer data being deleted due to poor input parsing in a shell script before, so I never recommend quick and dirty methods that may be fragile in this way. If you're doing some one-off processing, see the other answers for suggestions, but I still highly recommend just using an existing tested JSON parser. Historical notes This answer originally recommended jsawk, which should still work, but is a little more cumbersome to use than jq, and depends on a standalone JavaScript interpreter being installed which is less common than a Python interpreter, so the above answers are probably preferable: curl -s 'https://api.github.com/users/lambda' | jsawk -a 'return this.name' This answer also originally used the Twitter API from the question, but that API no longer works, making it hard to copy the examples to test out, and the new Twitter API requires API keys, so I've switched to using the GitHub API which can be used easily without API keys. The first answer for the original question would be: curl 'http://twitter.com/users/username.json' | jq -r '.text'"}
{"text": "Question: How do I clone a specific Git branch?\nAnswer: git clone --single-branch --branch"}
{"text": "Question: Does using virtualenvwrapper with Python3.3 mean I cannot (or should not) be using pyvenv? Virtualenvwrapper is a user-friendly shell around Python's virtualenv. Python 3.3 ships with pyvenv built into the standard library, which aims to supercede virtualenv. But if I install Virtualenvwrapper on Python3.3, it still installs virtualenv, leading me to believe it doesn't use 'pyvenv' under the covers. Presumably this doesn't really matter - if I wish to use virtualenvwrapper on Python3.3 I should happily let it use virtualenv instead of pyvenv, and will (for the moment) suffer no ill effects?\nAnswer: Sorry this answer is a bit delayed. pyvenv does not aim to supersede virtualenv, in fact virtualenv in Python 3 depends on the standard library venv module. The pyvenv command creates an absolutely minimal virtual environment into which other packages can be installed. The Python 3 version of virtualenv actually subclasses the standard library's implementation and provides hooks to automatically install setuptools and pip into the environment which pyvenv doesn't do on it's own. As far as I know virtualenvwrapper depends on virtualenv only because the mkvirtualenv or mkproject commands allow you to specify packages to be installed into the new environment, this only works because virtualenv will have already installed setuptools and pip. So to answer your question I believe you should be able to use virtualenvwrapper on environments created by pyvenv as long as you follow virtualenvwrapper's conventions for where to put things and you either manually install setuptools and pip into the environment or don't use any package management features of virtualenvwrapper."}
{"text": "Question: How to use Pigz with Tar So I am using pigz in tar with tar --use-compress-program=pigz and this works but it doesn't use all of my processors, and I'd like to make sure it's recursive (-r) and using (-9 compression level). I read through Utilizing multi core for tar+gzip/bzip compression/decompression but it doesn't note anywhere to add additional commands in that format, and I couldn't find anything in the man page for either program for additional swithed. Thanks, Cam\nAnswer: Mark Adler's top voted answer on the SO link that you included in your question does provide a solution for specifying compression-level as well as number of processors to use: tar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz See : https://stackoverflow.com/a/12320421"}
{"text": "Question: How do I iterate over a range of numbers defined by variables in Bash?\nAnswer: for i in $(seq 1 $END); do echo $i;)"}
{"text": "Question: How do I use different Python version in venv from standard library? (Not virtualenv!) I have installed Python 3.4.0 and created virtual environment with python -m venv myenv. How can I change Python version in my virtual environment? Documentation says: Each virtual environment has its own Python binary (allowing creation of environments with various Python versions) and can have its own independent set of installed Python packages in its site directories. UPDATE Please, note that I ask about venv from standard library, not about virtualenv. Let me provide some links. This is PEP 405. http://legacy.python.org/dev/peps/pep-0405/ Python venv. http://docs.python.org/3.4/library/venv.html Virtualenv. http://www.virtualenv.org/en/latest/ I don't see something like a --python flag in venv. Are venv and virtualenv absolutely similar? Is venv is so unpopular and no one uses it so that virtualenv remains the standard?\nAnswer: It's simply impossible. To create a Python venv of a specific Python version, we need this specific version. Obviously, a Python interpreter doesn't \"include\" all the previous versions with their behavior. Python 3.4.1 cannot contain Python 2.7.8 executable anywhere inside."}
{"text": "Question: How do I undo 'git add' before commit?\nAnswer: git reset"}
{"text": "Question: How to Remove a file from a Git repository without deleting it from the local filesystem ?\nAnswer: So, for a single file: git rm --cached file_to_remove.txt and for a single directory: git rm --cached -r directory_to_remove"}
{"text": "Question: completely self-contained virtual environment I create a python3 virtual environment (explicitly avoiding symlinks, with --copies): \u00bb python3 -m venv --without-pip --copies venv This is my complete virtual environment now: \u00bb tree venv/ venv/ \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 activate \u2502 \u251c\u2500\u2500 activate.csh \u2502 \u251c\u2500\u2500 activate.fish \u2502 \u251c\u2500\u2500 python \u2502 \u2514\u2500\u2500 python3 \u251c\u2500\u2500 include \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 python3.4 \u2502 \u2514\u2500\u2500 site-packages \u251c\u2500\u2500 lib64 -> lib \u2514\u2500\u2500 pyvenv.cfg I disable the PYTHONPATH, to make sure nothing is leaking from outside: \u00bb PYTHONPATH=\"\" Activate the venv: \u00bb source venv/bin/activate Verify that activate has not polluted my PYTHONPATH: \u00bb echo $PYTHONPATH (blank, as expected) I am using the right python: \u00bb which python /foo/bar/venv/bin/python But the system modules are still being accessed: \u00bb python Python 3.4.3 (default, Oct 14 2015, 20:28:29) [GCC 4.8.4] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import unittest >>> print(unittest) >>> I would expect the import unittest statement to fail, since the virtual environment has no such module. I would like to know: Why are system packages accessed when in a virtualenv? How can I create a completely self-contained virtual environment?\nAnswer: If I recall correctly the core system packages are symlinked, so they are the same files (partly to keep the size of the virtualenv down). The default is not to include the site-packages directory, so it won't access 3rd party libraries that have been installed. If you want to truly isolated and self-contained virtual environment, you might be better off looking at docker. Virtualenv is really more of a lightweight way of managing different 3rd party installed packages for different apps. EDIT: It looks like --always-copy doesn't actually always copy all files: virtualenv doesn't copy all .py files from the lib/python directory Digging into the source and it looks like there's a smallish set of modules that are deemed to be \"required\" and these are the ones that are copied: https://github.com/pypa/virtualenv/blob/ac4ea65b14270caeac56b1e1e64c56928037ebe2/virtualenv.py#L116 Edit 2: You can see that the old python directories still appear in the sys.path, but after the directories for the virtualenv itself: >>> import sys >>> sys.path ['', '/home/john/venv/lib/python2.7', '/home/john/venv/lib/python2.7/plat-linux2', '/home/john/venv/lib/python2.7/lib-tk', '/home/john/venv/lib/python2.7/lib-old', '/home/john/venv/lib/python2.7/lib-dynload', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-linux2', '/usr/lib/python2.7/lib-tk', '/home/john/venv/local/lib/python2.7/site-packages', '/home/john/venv/lib/python2.7/site-packages']"}
{"text": "Question: Deleting files after adding to tar archive Can GNU tar add many files to an archive, deleting each one as it is added? This is useful when there is not enough disk space to hold both the entire tar archive and the original files - and therefore it is not possible to simply manually delete the files after creating an archive in the usual way.\nAnswer: With GNU tar, use the option --remove-files."}
{"text": "Question: How to extract a single file from tar to a different directory? I know that I can use following command to extract a single file to the current working directory (assume I have a tar file named test.tar and a file named testfile1 and testfile2 are inside it): $tar xvf test.tar testfile1 And I can use -C option to extract files to another directory: $tar xvf test.tar -C anotherDirectory/ When I incorporate the above two techniques together, I suppose that I can extract a single file to another directory. $ tar xvf test.tar testfile1 -C anotherDirectory/ But the result is I can only extract the testfile1 to the current working directory, rather than the anotherDirectory. I want to know how can I extract a single file from tar to a different directory?\nAnswer: The problem is that your arguments are in incorrect order. The single file argument must be last. E.g. $ tar xvf test.tar -C anotherDirectory/ testfile1 should do the trick. PS: You should have asked this question on superuser instead of SO"}
{"text": "Question: Why 'python3 -m venv myenv' installs older version of pip into myenv than any version of pip I can find anywhere on the system? This is not causing me any problem that I can't solve by activating the virtual environment and running pip install -U pip, but I always wonder where the older version of pip is coming from. I'm using OS X 10.7.5. When I create a virtual environment using either pyvenv-3.4 myenv or python3 -m venv myenv, the version of pip that is installed inside the virtual environment is 6.0.8, but I have upgraded my global pip to 6.1.1. Here is a terminal session demonstrating what I mean: $ python3 -m venv myenv $ myenv/bin/pip -V pip 6.0.8 from /Users/dust/Desktop/myenv/lib/python3.4/site-packages (python 3.4) Here is what I would like to occur: $ source myenv/bin/activate (myenv)$ pip -V UPDATED SYSTEM VERSION HERE WOULD BE NICE I can't find a pip 6.0.8 anywhere else, other than what is created inside virtual environments. Here are the outputs of various commands that I have use to try and figure this out: $ which pip /Library/Frameworks/Python.framework/Versions/3.4/bin/pip $ which pip3 /Library/Frameworks/Python.framework/Versions/3.4/bin/pip3 $ pip -V pip 6.1.1 from /Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages (python 3.4) $ pip3 -V pip 6.1.1 from /Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages (python 3.4) I even tried using find: $ find / -type f -name pip 2>&1 | awk '! /^f.*$/' /Library/Frameworks/Python.framework/Versions/3.4/bin/pip /usr/local/bin/pip $ find / -type f -name pip3 2>&1 | awk '! /^f.*$/' /Library/Frameworks/Python.framework/Versions/3.4/bin/pip3 I thought maybe that the /usr/local/bin/pip might have been the culprit, but no: $ /usr/local/bin/pip -V pip 6.1.1 from /Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg (python 2.7) Hmm. Perhaps the OS X python has it? $ /usr/bin/python >>> import pip >>> pip.__version__ '6.1.1' 6.1.1 is reported no matter which distribution of python I ask, whether it be OS X's 2.7.1, python.org's 2.7.9, or python.org's 3.4.3. Is it possible (or advisable) to update the version of pip that gets put into a virtual environment?\nAnswer: I face the same issue, running OSX 10.10.2 and python 3.4.2. Most recently I created a virtual environment in a debian wheezy machine with python 3.4.3 and also ended up with an older version of pip than available. had to upgrade pip. I've been upgrading pip within the virtual environment to 6.1.1 from 6.0.8 manually, because I'm o.c.d about software library versions that way - and yes, I am upgrading my python 3 version to 3.4.3 right now. Anyway, my system's python3-pip is the latest version 6.1.1, so I've also wondered why pyvenv creates a new virtual environment and loads it with old pip. I haven't noticed anything bad happen in any of the virtual environments due to upgrading pip, (but on the flip side, I haven't noticed anything good either) Apparently the new pip is faster -- didn't notice, and outputs less junk on successful installs because user's don't care -- also didn't notice, probably because i'm one of those that don't care, and also comes with a state-of-the art coffee machine capable of latte art to boot!!! -- still waiting on sudo pip install latte to finish :( So, to answer your question, it is definitely possible, and probably advisable to upgrade, because apparently the new pip fixes some bugs and goes faster, but I guess the speed up isn't that major, and the bug fixes don't affect all that many people (I've never faced a bug with my usage of the old pip). You can link to system site-packages using the flag --system-site-packages when you create a new virtual environment, like this pyvenv myenv --system-site-packages This will link to your system wide version of pip, and would remove the annoyance that is manually upgrading pip on every virtual environment, but if you do this, then is your virtual environment all that virtual? update: following my rant above, I went into the venv package's source to dig. pip is set up by a method called _setup_pip in the file __init__.py, line 248 def _setup_pip(self, context): \"\"\"Installs or upgrades pip in a virtual environment\"\"\" # We run ensurepip in isolated mode to avoid side effects from # environment vars, the current directory and anything else # intended for the global Python environment cmd = [context.env_exe, '-Im', 'ensurepip', '--upgrade', '--default-pip'] subprocess.check_output(cmd, stderr=subprocess.STDOUT) So, venv seems to be calling ensurepip from the shell using the subprocess module. One more minute of google-fu gave me this from the documentation for ensurepip. ensurepip.version() Returns a string specifying the bundled version of pip that will be installed when bootstrapping an environment. So, from the command line, the following code: $ python3 -c 'import ensurepip; print(ensurepip.version())' 6.0.8 displays my current pip that will be bootstrapped with ensurepip. I guess we're stuck with the old version of pip for every new install until ensurepip gets upgraded, as I can't find a way to upgrade the version of pip that comes with ensurepip"}
{"text": "Question: Use grep --exclude/--include syntax to not grep through certain files I'm looking for the string foo= in text files in a directory tree. It's on a common Linux machine, I have bash shell: grep -ircl \"foo=\" * In the directories are also many binary files which match \"foo=\". As these results are not relevant and slow down the search, I want grep to skip searching these files (mostly JPEG and PNG images). How would I do that? I know there are the --exclude=PATTERN and --include=PATTERN options, but what is the pattern format? The man page of grep says: --include=PATTERN Recurse in directories only searching file matching PATTERN. --exclude=PATTERN Recurse in directories skip file matching PATTERN. Searching on grep include, grep include exclude, grep exclude and variants did not find anything relevant If there's a better way of grepping only in certain files, I'm all for it; moving the offending files is not an option. I can't search only certain directories (the directory structure is a big mess, with everything everywhere). Also, I can't install anything, so I have to do with common tools (like grep or the suggested find).\nAnswer: Use the shell globbing syntax: grep pattern -r --include=\\*.cpp --include=\\*.h rootdir The syntax for --exclude is identical. Note that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well). Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted. Update 2021-03-04 I've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant. The original example was: grep pattern -r --include=\\*.{cpp,h} rootdir to search through all .cpp and .h files rooted in the directory rootdir."}
{"text": "Question: Can 2 VS Code windows for 2 git clones each work with their own venv? Background (adjacent to my question) This doesn't happen often, but sometimes, I get carried away with a proof of concept effort and end up too far down a rapid prototyping rabbit hole, with a bunch of work I want to keep, so I break up and organize my work into a stacked series of small branches using a git squash merge, cherry-pick, followed by a repeating series of git adds, commits, pushes, and branching. As I create an organized commit history to pick related changes for a coherent commit history, I write tests and do linting, which sometimes requires some code changes along the way, so the further I get out of the disorganized mess, the more the new series of stacked branches diverge. Since this creates conflicts, I figured out a way to keep these divergent commit histories flowing smoothly... I have 1 repo clone containing the messy rapid prototyping code where I organize related changes into separate branches, which I push to github. I then have a second clone where I pull those branches to lint, test, tweak, then rebase off the previously linted, tested, and tweaked code from the previous branch. Question I know some of you are probably shaking your heads right now, but it flows surprisingly smoothly this way. The only problem is that I use 2 VSCode windows and the one for the secondary clone where I do the clean-up cannot resolve the imports. It's really just a minor annoyance, but I tried to figure it out today by setting up the secondary clone with its own venv, but it didn't work, so this is what I see (secondary window above and primary window below): I imagine this is due to the fact that when I launch VSCode for the primary clone, it uses its venv... for both windows. But the 2 clones are in different directories, so it cannot \"find\" the imports?\nAnswer: OK. It was pretty simple. I just had to: On the command line: activate my venv: source .venv/bin/activate code . command-shift-p Click \"Python: select interpreter\" Click \"Enter Interpreter path\" Paste and hit Enter I was mistakenly guessing that just activating the environment (either in the terminal from which I launch vscode or in vscode's terminal) was enough. I probably did this for the primary clone long ago and just forgot."}
{"text": "Question: Get line number while using grep I am using grep recursive to search files for a string, and all the matched files and the lines containing that string are printed on the terminal. But is it possible to get the line numbers of those lines too? Example: presently, I get /var/www/file.php: $options = \"this.target\", but I am trying to get /var/www/file.php: 1142 $options = \"this.target\";, well where 1142 would be the line number containing that string. The syntax I am using to grep recursively is sudo grep -r 'pattern' '/var/www/file.php' How do we get results for not equal to a pattern? Like all the files, but not the ones having a certain string.\nAnswer: grep -n SEARCHTERM file1 file2 ..."}
{"text": "Question: What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc? Python 3.3 includes in its standard library the new package venv. What does it do, and how does it differ from all the other packages that match the regex (py)?(v|virtual|pip)?env?\nAnswer: This is my personal recommendation for beginners: start by learning virtualenv and pip, tools which work with both Python 2 and 3 and in a variety of situations, and pick up other tools once you start needing them. Now on to answer the question: what is the difference between these similarly named things: venv, virtualenv, etc? PyPI packages not in the standard library: virtualenv is a very popular tool that creates isolated Python environments for Python libraries. If you're not familiar with this tool, I highly recommend learning it, as it is a very useful tool. It works by installing a bunch of files in a directory (eg: env/), and then modifying the PATH environment variable to prefix it with a custom bin directory (eg: env/bin/). An exact copy of the python or python3 binary is placed in this directory, but Python is programmed to look for libraries relative to its path first, in the environment directory. It's not part of Python's standard library, but is officially blessed by the PyPA (Python Packaging Authority). Once activated, you can install packages in the virtual environment using pip. pyenv is used to isolate Python versions. For example, you may want to test your code against Python 2.7, 3.6, 3.7 and 3.8, so you'll need a way to switch between them. Once activated, it prefixes the PATH environment variable with ~/.pyenv/shims, where there are special files matching the Python commands (python, pip). These are not copies of the Python-shipped commands; they are special scripts that decide on the fly which version of Python to run based on the PYENV_VERSION environment variable, or the .python-version file, or the ~/.pyenv/version file. pyenv also makes the process of downloading and installing multiple Python versions easier, using the command pyenv install. pyenv-virtualenv is a plugin for pyenv by the same author as pyenv, to allow you to use pyenv and virtualenv at the same time conveniently. However, if you're using Python 3.3 or later, pyenv-virtualenv will try to run python -m venv if it is available, instead of virtualenv. You can use virtualenv and pyenv together without pyenv-virtualenv, if you don't want the convenience features. virtualenvwrapper is a set of extensions to virtualenv (see docs). It gives you commands like mkvirtualenv, lssitepackages, and especially workon for switching between different virtualenv directories. This tool is especially useful if you want multiple virtualenv directories. pyenv-virtualenvwrapper is a plugin for pyenv by the same author as pyenv, to conveniently integrate virtualenvwrapper into pyenv. pipenv aims to combine Pipfile, pip and virtualenv into one command on the command-line. The virtualenv directory typically gets placed in ~/.local/share/virtualenvs/XXX, with XXX being a hash of the path of the project directory. This is different from virtualenv, where the directory is typically in the current working directory. pipenv is meant to be used when developing Python applications (as opposed to libraries). There are alternatives to pipenv, such as poetry, which I won't list here since this question is only about the packages that are similarly named. Standard library: pyvenv (not to be confused with pyenv in the previous section) is a script shipped with Python 3.3 to 3.7. It was removed from Python 3.8 as it had problems (not to mention the confusing name). Running python3 -m venv has exactly the same effect as pyvenv. venv is a package shipped with Python 3, which you can run using python3 -m venv (although for some reason some distros separate it out into a separate distro package, such as python3-venv on Ubuntu/Debian). It serves the same purpose as virtualenv, but only has a subset of its features (see a comparison here). virtualenv continues to be more popular than venv, especially since the former supports both Python 2 and 3."}
{"text": "Question: grep without showing path/file:line How do you grep and only return the matching line? i.e. The path/filename is omitted from the results. In this case I want to look in all .bar files in the current directory, searching for the term FOO find . -name '*.bar' -exec grep -Hn FOO {} \\;\nAnswer: No need to find. If you are just looking for a pattern within a specific directory, this should suffice: grep -h FOO /your/path/*.bar Where -h is the parameter to hide the filename, as from man grep: -h, --no-filename Suppress the prefixing of file names on output. This is the default when there is only one file (or only standard input) to search. Note that you were using -H, --with-filename Print the file name for each match. This is the default when there is more than one file to search."}
{"text": "Question: How do I find files that do not contain a given string pattern? How do I find out the files in the current directory which do not contain the word foo (using grep)?\nAnswer: The following command gives me all the files that do not contain the pattern foo: find . -not -ipath '.*svn*' -exec grep -H -E -o -c \"foo\" {} \\; | grep 0"}
{"text": "Question: How to check if a variable is set in Bash How do I know if a variable is set in Bash? For example, how do I check if the user gave the first parameter to a function? function a { # if $1 is set ? }\nAnswer: if [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi where ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise."}
{"text": "Question: How to check if a file contains a specific string using Bash I want to check if a file contains a specific string or not in bash. I used this script, but it doesn't work: if [[ 'grep 'SomeString' $File' ]];then # Some Actions fi What's wrong in my code?\nAnswer: if grep -q SomeString \"$File\"; then Some Actions # SomeString was found fi You don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found. The grep command returns 0 or 1 in the exit code depending on the result of search. 0 if something was found; 1 otherwise. $ echo hello | grep hi ; echo $? 1 $ echo hello | grep he ; echo $? hello 0 $ echo hello | grep -q he ; echo $? 0 You can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false. $ if /bin/true; then echo that is true; fi that is true $ if /bin/false; then echo that is true; fi $ As you can see you run here the programs directly. No additional [] or [[]]."}
{"text": "Question: Fast way of finding lines in one file that are not in another? I have two large files (sets of filenames). Roughly 30.000 lines in each file. I am trying to find a fast way of finding lines in file1 that are not present in file2. For example, if this is file1: line1 line2 line3 And this is file2: line1 line4 line5 Then my result/output should be: line2 line3 This works: grep -v -f file2 file1 But it is very, very slow when used on my large files. I suspect there is a good way to do this using diff, but the output should be just the lines, nothing else, and I cannot seem to find a switch for that. Can anyone help me find a fast way of doing this, using bash and basic Linux binaries? EDIT: To follow up on my own question, this is the best way I have found so far using diff: diff file2 file1 | grep '^>' | sed 's/^>\\ //' Surely, there must be a better way?\nAnswer: You can achieve this by controlling the formatting of the old/new/unchanged lines in GNU diff output: diff --new-line-format=\"\" --unchanged-line-format=\"\" file1 file2 The input files should be sorted for this to work. With bash (and zsh) you can sort in-place with process substitution : diff --new-line-format=\"\" --unchanged-line-format=\"\" <(sort file1) <(sort file2) In the above new and unchanged lines are suppressed, so only changed (i.e. removed lines in your case) are output. You may also use a few diff options that other solutions don't offer, such as -i to ignore case, or various whitespace options (-E, -b, -v etc) for less strict matching. Explanation The options --new-line-format, --old-line-format and --unchanged-line-format let you control the way diff formats the differences, similar to printf format specifiers. These options format new (added), old (removed) and unchanged lines respectively. Setting one to empty \"\" prevents output of that kind of line. If you are familiar with unified diff format, you can partly recreate it with: diff --old-line-format=\"-%L\" --unchanged-line-format=\" %L\" \\ --new-line-format=\"+%L\" file1 file2 The %L specifier is the line in question, and we prefix each with \"+\" \"-\" or \" \", like diff -u (note that it only outputs differences, it lacks the --- +++ and @@ lines at the top of each grouped change). You can also use this to do other useful things like number each line with %dn. The diff method (along with other suggestions comm and join) only produce the expected output with sorted input, though you can use to sort in place. Here's a simple awk (nawk) script (inspired by the scripts linked-to in Konsolebox's answer) which accepts arbitrarily ordered input files, and outputs the missing lines in the order they occur in file1. # output lines in file1 that are not in file2 BEGIN { FS=\"\" } # preserve whitespace (NR==FNR) { ll1[FNR]=$0; nl1=FNR; } # file1, index by lineno (NR!=FNR) { ss2[$0]++; } # file2, index by string END { for (ll=1; ll<=nl1; ll++) if (!(ll1[ll] in ss2)) print ll1[ll] } This stores the entire contents of file1 line by line in a line-number indexed array ll1[], and the entire contents of file2 line by line in a line-content indexed associative array ss2[]. After both files are read, iterate over ll1 and use the in operator to determine if the line in file1 is present in file2. (This will have have different output to the diff method if there are duplicates.) In the event that the files are sufficiently large that storing them both causes a memory problem, you can trade CPU for memory by storing only file1 and deleting matches along the way as file2 is read. BEGIN { FS=\"\" } (NR==FNR) { # file1, index by lineno and string ll1[FNR]=$0; ss1[$0]=FNR; nl1=FNR; } (NR!=FNR) { # file2 if ($0 in ss1) { delete ll1[ss1[$0]]; delete ss1[$0]; } } END { for (ll=1; ll<=nl1; ll++) if (ll in ll1) print ll1[ll] } The above stores the entire contents of file1 in two arrays, one indexed by line number ll1[], one indexed by line content ss1[]. Then as file2 is read, each matching line is deleted from ll1[] and ss1[]. At the end the remaining lines from file1 are output, preserving the original order. In this case, with the problem as stated, you can also divide and conquer using GNU split (filtering is a GNU extension), repeated runs with chunks of file1 and reading file2 completely each time: split -l 20000 --filter='gawk -f linesnotin.awk - file2' < file1 Note the use and placement of - meaning stdin on the gawk command line. This is provided by split from file1 in chunks of 20000 line per-invocation. For users on non-GNU systems, there is almost certainly a GNU coreutils package you can obtain, including on OSX as part of the Apple Xcode tools which provides GNU diff, awk, though only a POSIX/BSD split rather than a GNU version."}
{"text": "Question: How can I have grep not print out 'No such file or directory' errors? I'm grepping through a large pile of code managed by git, and whenever I do a grep, I see piles and piles of messages of the form: > grep pattern * -R -n whatever/.git/svn: No such file or directory Is there any way I can make those lines go away?\nAnswer: You can use the -s or --no-messages flag to suppress errors. -s, --no-messages suppress error messages grep pattern * -s -R -n"}
{"text": "Question: Capturing Groups From a Grep RegEx I've got this script in sh (macOS 10.6) to look through an array of files: files=\"*.jpg\" for f in $files do echo $f | grep -oEi '[0-9]+_([a-z]+)_[0-9a-z]*' name=$? echo $name done So far $name merely holds 0, 1 or 2, depending on if grep found that the filename matched the matter provided. What I'd like is to capture what's inside the parens ([a-z]+) and store that to a variable. I'd like to use grep only, if possible. If not, please no Python or Perl, etc. sed or something like it \u2013 I would like to attack this from the *nix purist angle.\nAnswer: If you're using Bash, you don't even have to use grep: files=\"*.jpg\" regex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally for f in $files # unquoted in order to allow the glob to expand do if [[ $f =~ $regex ]] then name=\"${BASH_REMATCH[1]}\" echo \"${name}.jpg\" # concatenate strings name=\"${name}.jpg\" # same thing stored in a variable else echo \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files fi done It's better to put the regex in a variable. Some patterns won't work if included literally. This uses =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match. side note #1 regarding regex anchors: You should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for: 123_abc_d4e5 xyz123_abc_d4e5 123_abc_d4e5.xyz xyz123_abc_d4e5.xyz To eliminate the second and fourth examples, make your regex like this: ^[0-9]+_([a-z]+)_[0-9a-z]* which says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this: ^[0-9]+_([a-z]+)_[0-9a-z]*$ then the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well. side note #2 regarding grep and the \\K operator: If you have GNU grep (around 2.5 or later, I think, when the \\K operator was added): name=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg The \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (? - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}). The (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result. In order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant. The regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured."}
{"text": "Question: How to echo shell commands as they are executed In a shell script?\nAnswer: set -x or set -o xtrace expands variables and prints a little + sign before the line. set -v or set -o verbose does not expand the variables before printing. Use set +x and set +v to turn off the above settings. On the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script."}
{"text": "Question: Undo a Git merge that hasn't been pushed yet?\nAnswer: With git reflog check which commit is one prior the merge, Then you can reset it using: git reset --hard commit_sha"}
{"text": "Question: Create a tar.xz in one command?\nAnswer: Use the -J compression option for xz. And remember to man tar :) tar cfJ"}
{"text": "Question: Utilizing multi core for tar+gzip/bzip compression/decompression I normally compress using tar zcvf and decompress using tar zxvf (using gzip due to habit). I've recently gotten a quad core CPU with hyperthreading, so I have 8 logical cores, and I notice that many of the cores are unused during compression/decompression. Is there any way I can utilize the unused cores to make it faster?\nAnswer: You can use pigz instead of gzip, which does gzip compression on multiple cores. Instead of using the -z option, you would pipe it through pigz: tar cf - paths-to-archive | pigz > archive.tar.gz By default, pigz uses the number of available cores, or eight if it could not query that. You can ask for more with -p n, e.g. -p 32. pigz has the same options as gzip, so you can request better compression with -9. E.g. tar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz"}
{"text": "Question: How to modify existing, unpushed commit messages?\nAnswer: git commit --amend"}
{"text": "Question: How do I turn off the output from tar commands on Unix? The below is the code I am currently using... I just need the option to switch off the output. tar -zxvf tmp.tar.gz -C ~/tmp1\nAnswer: Just drop the option v. -v is for verbose. If you don't use it then it won't display: tar -zxf tmp.tar.gz -C ~/tmp1"}
{"text": "Question: venv doesn't create activate script python3 When trying to create a virtulenv using venv with python 3 on ubuntu it isn\u2019t creating an activate script. It conitunally exits with an error 1. Following docs and other posts on SO such as https://stackoverflow.com/a/19848770 I have tried creating it 2 different ways. sayth@sayth-TravelMate-5740G:~/scripts$ python3 -m venv test4 Error: Command '['/home/sayth/scripts/test4/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1 sayth@sayth-TravelMate-5740G:~/scripts$ source test4/bin/activate bash: test4/bin/activate: No such file or directory sayth@sayth-TravelMate-5740G:~/scripts$ ls test4/bin/ python python3 or sayth@sayth-TravelMate-5740G:~/scripts$ pyvenv-3.4 test5 Error: Command '['/home/sayth/scripts/test5/bin/python3.4', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1 sayth@sayth-TravelMate-5740G:~/scripts$ ls test5/bin/ python python3 python3.4 How can I get it to fully create a venv? If I do it as below with stil no success unsure what the issue is? sayth@sayth-TravelMate-5740G:~/scripts$ python3 -Im venv panda3 Error: Command '['/home/sayth/scripts/panda3/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1 sayth@sayth-TravelMate-5740G:~/scripts$ python3 -m venv panda4 Error: Command '['/home/sayth/scripts/panda4/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1\nAnswer: Looks like you are using Ubuntu 14.04. It was shipped with a broken pyvenv. There is a simple work around to create venv using Python 3 1. Create venv without pip python3 -m venv --without-pip test4 or pyvenv-3.4 --without-pip test4 2. Get pip in your env source test4/bin/activate curl https://bootstrap.pypa.io/get-pip.py | python deactivate source test4/bin/activate or pyvenv-3.4 --without-pip myvenv source ./myvenv/bin/activate wget https://pypi.python.org/packages/source/s/setuptools/setuptools-3.4.4.tar.gz tar -vzxf setuptools-3.4.4.tar.gz cd setuptools-3.4.4 python setup.py install cd .. wget https://pypi.python.org/packages/source/p/pip/pip-1.5.6.tar.gz tar -vzxf pip-1.5.6.tar.gz cd pip-1.5.6 python setup.py install cd .. deactivate source ./myvenv/bin/activate Source: HackerNews, AskUbuntu"}
{"text": "Question: Negative matching using grep (match lines that do not contain foo) How do I match all lines not matching a particular pattern using grep? I tried this: grep '[^foo]'\nAnswer: grep -v is your friend: grep --help | grep invert -v, --invert-match select non-matching lines Also check out the related -L (the complement of -l). -L, --files-without-match only print FILE names containing no match"}
{"text": "Question: How to tar certain file types in all subdirectories? I want to tar and all .php and .html files in a directory and its subdirectories.\nAnswer: find ./someDir -name \"*.php\" -o -name \"*.html\" | tar -cf my_archive -T -"}
{"text": "Question: How could I commit only 15 lines out of 30 lines that have been changed in a file?\nAnswer: You can use: git add --patch or for short: git add -p Git will break down your file into what it thinks are sensible (portions of the file). It will then prompt you with this question: Stage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]? Here is a description of each option: y stage this hunk for the next commit n do not stage this hunk for the next commit q quit; do not stage this hunk or any of the remaining hunks a stage this hunk and all later hunks in the file d do not stage this hunk or any of the later hunks in the file g select a hunk to go to / search for a hunk matching the given regex j leave this hunk undecided, see next undecided hunk J leave this hunk undecided, see next hunk k leave this hunk undecided, see previous undecided hunk K leave this hunk undecided, see previous hunk s split the current hunk into smaller hunks e manually edit the current hunk You can then edit the hunk manually by replacing +/- by # (thanks veksen) ? print hunk help If the file is not in the repository yet, you can first do git add -N . Afterwards you can go on with git add -p . Afterwards, you can use: git diff --staged to check that you staged the correct changes git reset -p to unstage mistakenly added hunks git commit -v to view your commit while you edit the commit message."}
{"text": "Question: How to check if a Unix .tar.gz file is a valid file without uncompressing?\nAnswer: What about just getting a listing of the tarball and throw away the output, rather than decompressing the file? tar -tzf my_tar.tar.gz >/dev/null"}
{"text": "Question: how can I find out which python virtual environment I am using? I have several virtual environment in my computer and sometimes I am in doubt about which python virtual environment I am using. Is there an easy way to find out which virtual environment I am connected to?\nAnswer: You can use sys.prefix to determine which virtualenv you're in. import sys print(sys.prefix) from the sys docs A string giving the site-specific directory prefix where the platform independent Python files are installed"}
{"text": "Question: reading tar file contents without untarring it, in python script I have a tar file which has number of files within it. I need to write a python script which will read the contents of the files and gives the count o total characters, including total number of letters, spaces, newline characters, everything, without untarring the tar file.\nAnswer: you can use getmembers() >>> import tarfile >>> tar = tarfile.open(\"test.tar\") >>> tar.getmembers() After that, you can use extractfile() to extract the members as file object. Just an example import tarfile,os import sys os.chdir(\"/tmp/foo\") tar = tarfile.open(\"test.tar\") for member in tar.getmembers(): f=tar.extractfile(member) content=f.read() print \"%s has %d newlines\" %(member, content.count(\"\\n\")) print \"%s has %d spaces\" % (member,content.count(\" \")) print \"%s has %d characters\" % (member, len(content)) sys.exit() tar.close() With the file object f in the above example, you can use read(), readlines() etc."}
{"text": "Question: Uncompress tar.gz file With the usage of wget command line I got a tar.gz file. I downloaded it in the root@raspberrypi. Is there any way to uncompress it in the /usr/src folder?\nAnswer: Use -C option of tar: tar zxvf .tar.gz -C /usr/src/ and then, the content of the tar should be in: /usr/src/"}
{"text": "Question: How to check if a string contains a substring in Bash I have a string in Bash: string=\"My string\" How can I test if it contains another string?\nAnswer: string='My long string' if [[ $string == *\"My long\"* ]]; then echo \"It's there!\" fi"}
{"text": "Question: Listing the content of a tar file or a directory only down to some level I wonder how to list the content of a tar file only down to some level? I understand tar tvf mytar.tar will list all files, but sometimes I would like to only see directories down to some level. Similarly, for the command ls, how do I control the level of subdirectories that will be displayed? By default, it will only show the direct subdirectories, but not go further.\nAnswer: tar tvf scripts.tar | awk -F/ '{if (NF<4) print }' drwx------ glens/glens 0 2010-03-17 10:44 scripts/ -rwxr--r-- glens/www-data 1051 2009-07-27 10:42 scripts/my2cnf.pl -rwxr--r-- glens/www-data 359 2009-08-14 00:01 scripts/pastebin.sh -rwxr--r-- glens/www-data 566 2009-07-27 10:42 scripts/critic.pl -rwxr-xr-x glens/glens 981 2009-12-16 09:39 scripts/wiki_sys.pl -rwxr-xr-x glens/glens 3072 2009-07-28 10:25 scripts/blacklist_update.pl -rwxr--r-- glens/www-data 18418 2009-07-27 10:42 scripts/sysinfo.pl Make sure to note, that the number is 3+ however many levels you want, because of the / in the username/group. If you just do tar tf scripts.tar | awk -F/ '{if (NF<3) print }' scripts/ scripts/my2cnf.pl scripts/pastebin.sh scripts/critic.pl scripts/wiki_sys.pl scripts/blacklist_update.pl scripts/sysinfo.pl it's only two more. You could probably pipe the output of ls -R to this awk script, and have the same effect."}
{"text": "Question: How to grep (search through) committed code in the Git history I have deleted a file or some code in a file sometime in the past. Can I search through the content (not just the commit messages)? A very poor solution is to grep the log: git log -p | grep However, this doesn't return the commit hash straight away. I played around with git grep to no avail.\nAnswer: You should use the pickaxe (-S) option of git log. To search for Foo: git log -SFoo -- path_containing_change git log -SFoo --since=2009.1.1 --until=2010.1.1 -- path_containing_change See Git history - find lost line by keyword for more. -S (named pickaxe) comes originally from a git diff option (Git v0.99, May 2005). Then -S (pickaxe) was ported to git log in May 2006 with Git 1.4.0-rc1. As Jakub Nar\u0119bski commented: this looks for differences that introduce or remove an instance of . It usually means \"revisions where you added or removed line with 'Foo'\". the --pickaxe-regex option allows you to use extended POSIX regex instead of searching for a string. Example (from git log): git log -S\"frotz\\(nitfol\" --pickaxe-regex As Rob commented, this search is case-sensitive - he opened a follow-up question on how to search case-insensitive. Hi Angel notes in the comments: Executing a git log -G --branches --all (the -G is same as -S but for regexes) does same thing as the accepted one (git grep $(git rev-list --all)), but it soooo much faster! The accepted answer was still searching for text after \u224810 minutes of me running it, whereas this one gives results after \u22484 seconds \ud83e\udd37\u200d\u2642\ufe0f. The output here is more useful as well"}
{"text": "Question: PyCharm venv failed: 'no such option: --build-dir' I'm doing a fresh install on a new Windows 10 laptop. I installed Python 3.9 and PyCharm Community 2020.2, then started a new project. In the project settings, I created a new project interpreter in a venv, inside the /venv folder. Everything looks to get set up correctly, but I can't install anything to the project interpreter. When I try to do so, e.g. when I try to install pandas or anything else, I get None-zero exit code (2) with the following message: Usage: D:\\MyProject\\project\\venv\\Scripts\\python.exe -m pip install [options] [package-index-options] ... D:\\MyProject\\project\\venv\\Scripts\\python.exe -m pip install [options] -r [package-index-options] ... D:\\MyProject\\project\\venv\\Scripts\\python.exe -m pip install [options] [-e] ... D:\\MyProject\\project\\venv\\Scripts\\python.exe -m pip install [options] [-e] ... D:\\MyProject\\project\\venv\\Scripts\\python.exe -m pip install [options] ... no such option: --build-dir When I go to the Terminal and just 'pip install pandas' per PyCharm's 'proposed solution', it installs fine, and pandas and its dependencies appear as usual in the list of installed modules in the interpreter. I've not encountered this before, and don't see anywhere in the settings where I can specify how exactly PyCharm will invoke pip in this situation.\nAnswer: PyCharm relies on --build-dir to install packages and the flag was removed in the latest pip 20.3. The fix for PyCharm is ready and will be released this week in 2020.3 release (and backported to 2020.2.5 and 2020.1.5). The workaround is to downgrade pip to the previous version - close PyCharm and run python -m pip install pip==20.2.4 in the terminal using the corresponding virtual environment. Update 1 2020.1.5 and 2020.2.5 with the fix were released - please update."}
{"text": "Question: What's the difference between \"virtualenv\" and \"-m venv\" in creating Python Virtual environments Sorry if I sound a bit foolish. I'm confused about this: What's the difference between the two: virtualenv myvenv and -m venv myvenv The first one works well for me in creating virtual environments while the other does not. I cd into my development directory and use virtualenv myvenv and it creates the virtual environment. But if I use -m venv myvenv, it just gives errors.\nAnswer: venv is a package shipped directly with python 3. So you don't need to pip install anything. virtualenv instead is an independent library available at https://virtualenv.pypa.io/en/stable/ and can be installed with pip. They solve the same problem and work in a very similar manner. If you use python3 I suggest to avoid any \"extra\" dependencies and just stick with venv. Your error is probably because you use Python2/pip2."}
{"text": "Question: How do I easily undo a git rebase?\nAnswer: The easiest way would be to find the head commit of the branch as it was immediately before the rebase started in the reflog... git reflog and to reset the current branch to it."}
{"text": "Question: \"venv activate\" doesn't not change my Python path I create a virtual environment (test_venv) and I activate it. So far, successful. HOWEVER, the path of the Python Interpreter doesn't change. I have illustrated the situation below. For clarification, the python path SHOULD BE ~/Desktop/test_venv/bin/python. >>> python3 -m venv Desktop/test_venv >>> source Desktop/test_venv/bin/activate (test_venv) >>> which python /usr/bin/python\nAnswer: It is not an answer specifically to your question, but it corresponds the title of the question. I faced similar problem and couldn't find solution on Internet. Maybe someone use my experience. I created virtual environment for my python project. Some time later my python interpreter also stopped changing after virtual environment activation. Similar to how you described. My problem was that I moved the project folder to a different directory some time ago. And if I return the folder to its original directory, then everything starts working again. There is following problem resolution. You save all package requirements (for example, using 'pip freeze' or 'poetry') and remove 'venv'-folder (or in your case 'test_venv'-folder). After that we create virtual environment again, activate it and install all requirements. This approach resolved my problem."}
{"text": "Question: What is makeinfo, and how do I get it? I'm trying to build GNU grep, and when I run make, I get: [snip] /bin/bash: line 9: makeinfo: command not found What is makeinfo, and how do I get it? (This is Ubuntu, if it makes a difference)\nAnswer: In (at least) Ubuntu when using bash, it tells you what package you need to install if you type in a command and its not found in your path. My terminal says you need to install 'texinfo' package. sudo apt-get install texinfo"}
{"text": "Question: Is there a simple shell command/script that supports excluding certain files/folders from being archived?\nAnswer: $ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz . Make sure to put --exclude before the source and destination items."}
{"text": "Question: How to convert a string to lower case in Bash\nAnswer: POSIX standard tr $ echo \"$a\" | tr '[:upper:]' '[:lower:]' hi all AWK $ echo \"$a\" | awk '{print tolower($0)}' hi all"}
{"text": "Question: Python venv env Fails - [WinError 2] The system cannot find the file specified I installed the latest version of Python 3.8.2 on a Windows 10 machine. I previously had Python 3.7, which I uninstalled and confirmed in the System PATH it was no longer referenced. After installing the latest version, I run through CMD as Admin: py -m venv env and I get this error: Error: [WinError 2] The system cannot find the file specified: 'C:\\Users\\test_user\\Documents\\app_test\\env' I know the Python Path is in the System Path environmental settings, but not specifically for the user (don't know if that makes a difference?). I have also tried to uninstall virtualenv using powershell and reinstaling, but have the same result. Any ideas on where else to look to solve this?\nAnswer: I found out that Windows Defender now has a feature that blocks access/changes to system files. It added my documents folder by default, somehow preventing me from creating any folders in CMD despite having Admin Access. I hope this helps someone else!! In short -- you may have to revise or disable your Windows 10 \"Ranswomare Security Protection\" setting to allow you to write files to your directories."}
{"text": "Question: How to determine the URL that a local Git repository was originally cloned from?\nAnswer: To obtain only the remote URL: git config --get remote.origin.url"}
{"text": "Question: Loop through an array of strings in Bash? I want to write a script that loops through 15 strings (array possibly?)\nAnswer: ## declare an array variable declare -a arr=(\"element1\" \"element2\" \"element3\") ## now loop through the above array for i in \"${arr[@]}\" do echo \"$i\" # or do whatever with individual element of the array done # You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also Also works for multi-line array declaration declare -a arr=(\"element1\" \"element2\" \"element3\" \"element4\" )"}
{"text": "Question: Create a .tar.bz2 file Linux On my Linux machine, I wish to create a .tar.bz2 file of a certain folder. Once I place myself in that folder (in the terminal), what do I type in the terminal command line to place the compressed folder in the home directory of my machine? Let's say I am in the folder /home/user/folder. In the folder \"folder\" are several files (txt, .c etc). How do I compress that folder of type .tar.bz2 and place it in my /home directory? In the /home/user/folder, I've tried sudo tar -cvjSf folder.tar.bz2 but get an error: tar: Cowardly refusing to create an empty archive\nAnswer: You are not indicating what to include in the archive. Go one level outside your folder and try: sudo tar -cvjSf folder.tar.bz2 folder Or from the same folder try sudo tar -cvjSf folder.tar.bz2 *"}
{"text": "Question: How do I tar a directory of files and folders without including the directory itself?\nAnswer: cd my_directory/ && tar -zcvf ../my_dir.tgz . && cd - should do the job in one line."}
{"text": "Question: Count all occurrences of a string in lots of files with grep I have a bunch of log files. I need to find out how many times a string occurs in all files. grep -c string * returns ... file1:1 file2:0 file3:0 ... Using a pipe I was able to get only files that have one or more occurrences: grep -c string * | grep -v :0 ... file4:5 file5:1 file6:2 ... How can I get only the combined count? (If it returns file4:5, file5:1, file6:2, I want to get back 8.)\nAnswer: cat * | grep -c string"}
{"text": "Question: How can I delete a remote tag? How can I delete a Git tag that has already been pushed?\nAnswer: You can push an 'empty' reference to the remote tag name: git push origin :tagname Or, more expressively, use the --delete option"}
{"text": "Question: How can files be added to a tarfile with Python, without adding the directory hierarchy? When I invoke add() on a tarfile object with a file path, the file is added to the tarball with directory hierarchy associated. In other words, if I unzip the tarfile the directories in the original directories hierarchy are reproduced. Is there a way to simply add a plain file, without directory info, so that untarring the resulting tarball produces a flat list of files?\nAnswer: You can use tarfile.addfile(), in the TarInfo object, which is the first parameter, you can specify a name that's different from the file you're adding. This piece of code should add /path/to/filename to the TAR file but will extract it as myfilename: tar.addfile(tarfile.TarInfo(\"myfilename.txt\"), open(\"/path/to/filename.txt\"))"}
{"text": "Question: View the change history of a file using Git versioning How do I view the history of an individual file with complete details of what has changed?\nAnswer: For a graphical view, use gitk: gitk [filename] To follow the file across file renames: gitk --follow [filename]"}
{"text": "Question: How to suppress binary file matching results in grep When using grep in Linux, the result often contains a lot of \"binary file XXX matches\", which I do not care about. How to suppress this part of the results, or how to exclude binary files in grep?\nAnswer: There are three options, that you can use. -I is to exclude binary files in grep. Other are for line numbers and file names. grep -I -n -H -I -- process a binary file as if it did not contain matching data; -n -- prefix each line of output with the 1-based line number within its input file -H -- print the file name for each match So this might be a way to run grep: grep -InH your-word *"}
{"text": "Question: How can I use grep to show just filenames on Linux? How can I use grep to show just file-names (no in-line matches) on Linux? I am usually using something like: find . -iname \"*php\" -exec grep -H myString {} \\; How can I just get the file-names (with paths), but without the matches? Do I have to use xargs? I didn't see a way to do this on my grep man page.\nAnswer: The standard option grep -l (that is a lowercase L) could do this. From the Unix standard: -l (The letter ell.) Write only the names of files containing selected lines to standard output. Pathnames are written once per file searched. If the standard input is searched, a pathname of (standard input) will be written, in the POSIX locale. In other locales, standard input may be replaced by something more appropriate in those locales. You also do not need -H in this case."}
{"text": "Question: How do I move my recent commits on master to a new branch, and reset master to before those commits were made?\nAnswer: You need to store uncommitted edits to your stash before doing this, using git stash. Once complete, you can retrieve the stashed uncommitted edits with git stash pop. git reset hard command will remove all changes! Moving to an existing branch If you want to move your commits to an existing branch, it will look like this: git checkout existingbranch git merge branchToMoveCommitFrom git checkout branchToMoveCommitFrom git reset --hard HEAD~3 # Go back 3 commits. You *will* lose uncommitted work. git checkout existingbranch Moving to a new branch."}
{"text": "Question: This command lists directories in the current path: ls -d */ What exactly does the pattern */ do?\nAnswer: */ is a pattern that matches all of the subdirectories in the current directory (* would match all files and subdirectories; the / restricts it to directories)."}
{"text": "Question: Find files and tar them (with spaces) ?\nAnswer: Use this: find . -type f -print0 | tar -czvf backup.tar.gz --null -T - It will: deal with files with spaces, newlines, leading dashes, and other funniness handle an unlimited number of files won't repeatedly overwrite your backup.tar.gz"}
{"text": "Question: How do I extract files without folder structure using tar I have a tar.gz-file with the following structure: folder1/img.gif folder2/img2.gif folder3/img3.gif I want to extract the image files without the folder hierarchy so the extracted result looks like: /img.gif /img2.gif /img3.gif I need to do this with a combination of Unix and PHP. Here is what I have so far, it works to extract them to the specified directory but keeps the folder hierarchy: exec('gtar --keep-newer-files -xzf images.tgz -C /home/user/public_html/images/',$ret);\nAnswer: You can use the --strip-components option of tar. --strip-components count (x mode only) Remove the specified number of leading path ele- ments. Pathnames with fewer elements will be silently skipped. Note that the pathname is edited after checking inclusion/exclu- sion patterns but before security checks. I create a tar file with a similar structure to yours: $tar -tf tarfolder.tar tarfolder/ tarfolder/file.a tarfolder/file.b $ls -la file.* ls: file.*: No such file or directory Then extracted by doing: $tar -xf tarfolder.tar --strip-components 1 $ls -la file.* -rw-r--r-- 1 ericgorr wheel 0 Jan 12 12:33 file.a -rw-r--r-- 1 ericgorr wheel 0 Jan 12 12:33 file.b"}
{"text": "Question: How do I delete a commit from a branch?\nAnswer: git reset --hard HEAD~1"}
{"text": "Question: How can I build a tar from stdin? How can I pipe information into tar specifying the names of the file?\nAnswer: Something like: tar cfz foo.tgz --files-from=- But keep in mind that this won't work for all possible filenames; you should consider the --null option and feed tar from find -print0. (The xargs example won't quite work for large file lists because it will spawn multiple tar commands.)"}
{"text": "Question: How does activating a python virtual environment modify sys.path? I create my python virtual environment using: python3 -m venv venv3 to activate, I source venv3/bin/activate. venv3/bin/activate doesn't appear to be all that complex: # This file must be used with \"source bin/activate\" *from bash* # you cannot run it directly deactivate () { # reset old environment variables if [ -n \"$_OLD_VIRTUAL_PATH\" ] ; then PATH=\"$_OLD_VIRTUAL_PATH\" export PATH unset _OLD_VIRTUAL_PATH fi if [ -n \"$_OLD_VIRTUAL_PYTHONHOME\" ] ; then PYTHONHOME=\"$_OLD_VIRTUAL_PYTHONHOME\" export PYTHONHOME unset _OLD_VIRTUAL_PYTHONHOME fi # This should detect bash and zsh, which have a hash command that must # be called to get it to forget past commands. Without forgetting # past commands the $PATH changes we made may not be respected if [ -n \"$BASH\" -o -n \"$ZSH_VERSION\" ] ; then hash -r fi if [ -n \"$_OLD_VIRTUAL_PS1\" ] ; then PS1=\"$_OLD_VIRTUAL_PS1\" export PS1 unset _OLD_VIRTUAL_PS1 fi unset VIRTUAL_ENV if [ ! \"$1\" = \"nondestructive\" ] ; then # Self destruct! unset -f deactivate fi } # unset irrelevant variables deactivate nondestructive VIRTUAL_ENV=\"/home/pi/django-test/venv3\" export VIRTUAL_ENV _OLD_VIRTUAL_PATH=\"$PATH\" PATH=\"$VIRTUAL_ENV/bin:$PATH\" export PATH # unset PYTHONHOME if set # this will fail if PYTHONHOME is set to the empty string (which is bad anyway) # could use `if (set -u; : $PYTHONHOME) ;` in bash if [ -n \"$PYTHONHOME\" ] ; then _OLD_VIRTUAL_PYTHONHOME=\"$PYTHONHOME\" unset PYTHONHOME fi if [ -z \"$VIRTUAL_ENV_DISABLE_PROMPT\" ] ; then _OLD_VIRTUAL_PS1=\"$PS1\" if [ \"x(venv3) \" != x ] ; then PS1=\"(venv3) $PS1\" else if [ \"`basename \\\"$VIRTUAL_ENV\\\"`\" = \"__\" ] ; then # special case for Aspen magic directories # see http://www.zetadev.com/software/aspen/ PS1=\"[`basename \\`dirname \\\"$VIRTUAL_ENV\\\"\\``] $PS1\" else PS1=\"(`basename \\\"$VIRTUAL_ENV\\\"`)$PS1\" fi fi export PS1 fi # This should detect bash and zsh, which have a hash command that must # be called to get it to forget past commands. Without forgetting # past commands the $PATH changes we made may not be respected if [ -n \"$BASH\" -o -n \"$ZSH_VERSION\" ] ; then hash -r fi I can see it modifying $PATH, and $PS1, creating a deactivate function, and even backing up old variables that it modifies so it can restore them when the user runs the deactivate function. All this makes sense. The one thing I don't see is where python's sys.path is modified. On my system, this is what I see: sys.path outside of virtual environment: ['', '/usr/lib/python35.zip', '/usr/lib/python3.5', '/usr/lib/python3.5/plat-arm-linux-gnueabihf', '/usr/lib/python3.5/lib-dynload', '/usr/local/lib/python3.5/dist-packages', '/usr/lib/python3/dist-packages'] sys.path inside of virtual environment: ['', '/usr/lib/python35.zip', '/usr/lib/python3.5', '/usr/lib/python3.5/plat-arm-linux-gnueabihf', '/usr/lib/python3.5/lib-dynload', '/home/pi/django-test/venv3/lib/python3.5/site-packages'] Clearly, sys.path gets modified at some point, somehow. This makes sense, since that's how python knows where to find the third-party python libraries that are installed. I would think that this is the main feature of the virtual environment, but I can't see where it gets set. I'm not trying to accomplish anything - mostly just curious.\nAnswer: sys.path is initiated in site.py, it is set using the relative path of sys.prefix, which is the path of python executable inside the virtual environment. if the virtual environment is created without option --system-site-packages, which is the default, the config value of key include-system-site-packages set to false in pyvenv.cfg . virtualenv has an identical option --system-site-packages, but it will write a file named no-global-site-packages.txt into the site dir of venv as a flag. during python startup, site.py is executed, it will check pyvenv.cfg config file to set sys.path: If \u201cpyvenv.cfg\u201d (a bootstrap configuration file) contains the key \u201cinclude-system-site-packages\u201d set to anything other than \u201ctrue\u201d (case-insensitive), the system-level prefixes will not be searched for site-packages; otherwise they will. if venv is created with virtualenv, site.py in venv is a modified version, it check the existence of file no-global-site-packages.txt, if this flag file not exists, system-wide site package path will be added to sys.path, which is infered from sys.real_prefix. update 2022: lastest virtualenv also use pyvenv.cfg. hope this could answer your question."}
{"text": "Question: abjad.show() issues \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" in Python Here's a basic, simple 'abjad' code that one can find in any 'abjad' documentation: import abjad n = abjad.Note(\"c'4\") abjad.show(n) And here's the full traceback produced by the above code: Traceback (most recent call last): File \"R:\\W\\y.py\", line 122, in abjad.show(n) File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\io.py\", line 672, in show result = illustrator() ^^^^^^^^^^^^^ File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\io.py\", line 75, in __call__ string = self.string or self.get_string() ^^^^^^^^^^^^^^^^^ File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\io.py\", line 152, in get_string return lilypond_file._get_lilypond_format() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\lilypondfile.py\", line 474, in _get_lilypond_format string = configuration.get_lilypond_version_string() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"R:\\W\\venv\\Lib\\site-packages\\abjad\\configuration.py\", line 388, in get_lilypond_version_string proc = subprocess.run(command, stdout=subprocess.PIPE) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Python3.12\\Lib\\subprocess.py\", line 548, in run with Popen(*popenargs, **kwargs) as process: ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Python3.12\\Lib\\subprocess.py\", line 1026, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"G:\\Python3.12\\Lib\\subprocess.py\", line 1538, in _execute_child hp, ht, pid, tid = _winapi.CreateProcess(executable, args, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ FileNotFoundError: [WinError 2] The system cannot find the file specified Note: I debugged 'subprocess.py' and saw that 'executable' was None, but I couldn't find what it was expected to be. I have installed abjad in a virtual environment, using both Python 3.10 and Python 3.12. And I have tried with both abjad versions, 3.19 & 3.21. Same story. I have installed hundreds of Python packages ... I can't remember this case evere having occurred. Any idea why's this happening?\nAnswer: Looking at the source of \"...\\abjad\\configuration.py\", line 388 (as pointed to by the error message), we have the following lines: command = [\"lilypond\", \"--version\"] proc = subprocess.run(command, stdout=subprocess.PIPE) So the process is trying to run the command \"lilypond --version\", and failing because Windows cannot find an executable file or command script with that name. You should check where that command is installed and ensure that its full path is added to your PATH environment variable."}
{"text": "Question: How to extract filename.tar.gz file I want to extract an archive named filename.tar.gz. Using tar -xzvf filename.tar.gz doesn't extract the file. it is gives this error: gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error exit delayed from previous errors\nAnswer: If file filename.tar.gz gives this message: POSIX tar archive, the archive is a tar, not a GZip archive. Unpack a tar without the z, it is for gzipped (compressed), only: mv filename.tar.gz filename.tar # optional tar xvf filename.tar"}
{"text": "Question: VS Code: Python Interpreter can't find my venv I've been stuck on this for a few days, kindly help me if you can. I have my venv folder on my root project folder. When I try to set the Python Interpreter, it shows me only the Python installed in my machine and not the one in my root folder. It was working fine until I formatted my PC and installed windows 10 64 bits. (was running on windows 7 64 bits prior) Things I have tried: Set the path manually via pythonPath and/or venvPath, in both workspace and user settings: \"python.pythonPath\": \"F:/Web Dev/Python/Django/project_x_v2/backend/venv/Scripts/python.exe\", \"python.venvPath\": \"F:/Web Dev/Python/Django/project_x_v2/backend/venv/Scripts/python.exe\", It shows me the correct location in the placeholder but I don't have the option to choose it from the dropdown list: Any ideas how I can solve this? Thank you very much. EDIT: In the image it shows \"python\", but I have corrected it to \"python.exe\" and it still does not work; ~\\AppData\\... is located in the disk C:\\ while my venv is located in the disk F:. I am not sure whether that is relevant though; The venv runs fine in the console.\nAnswer: The only solution I found was to delete the venv and recreate it. I followed these steps but I'll provide a brief summary for Windows: Activate your virtualenv. Go to the parent folder where your Virtual Environment is located and run venv\\scripts\\activate. Keep in mind that the first name \"venv\" can vary. Create a requirements.txt file. pip freeze > requirements.txt deactivate to exit the venv rm venv to delete the venv py -m venv venv to create a new one pip install -r requirements.txt to install the requirements."}
{"text": "Question: How do I tell if a file does not exist in Bash?\nAnswer: if [ ! -f /tmp/foo.txt ]; then echo \"File not found!\" fi"}
{"text": "Question: I keep getting a message to upgrade pip Whenever I create a venv, I get a message asking me to upgrade pip: You are using pip version 9.0.1, however version 18.0 is available. You should consider upgrading via the 'python -m pip install --upgrade pip' command. I can upgrade the Pip in that venv fine, after which it is up to date: C:\\Users\\mkupfer\\Python-Sandbox\\sibc-python-scripts>pip --version pip 18.0 from c:\\users\\mkupfer\\appdata\\local\\programs\\python\\python36-32\\lib\\sit e-packages\\pip (python 3.6) C:\\Users\\mkupfer\\Python-Sandbox\\sibc-python-scripts>pip3 --version pip 18.0 from c:\\users\\mkupfer\\appdata\\local\\programs\\python\\python36-32\\lib\\sit e-packages\\pip (python 3.6) C:\\Users\\mkupfer\\Python-Sandbox\\sibc-python-scripts>pip3 install --upgrade pip Requirement already up-to-date: pip in c:\\users\\mkupfer\\appdata\\local\\programs\\p ython\\python36-32\\lib\\site-packages (18.0) but if I create another venv, it will have the same issue. How can I make the upgrade permanent? I tried the advice at virtualenv use upgraded system default pip, but it does not solve the problem.\nAnswer: The issue seems to be that new virtual environments are using an old version of pip. Note that pip is installed from a source tarfile (or wheel) included with virtualenv, in the site-packages/virtualenv_support directory. $ ls -l /path/to/site-packages/virtualenv_support pip-9.1-py2.py3-none-any.whl A quick way to workaround the problem is to make sure you upgrade pip whenever you create a new virtualenv, like so: $ virtualenv venv $ venv/bin/pip install -U pip Alternatively, make sure you have the latest version of virtualenv. According to their release notes, virtualenv==16 is using pip==10. $ pip install -U virtualenv Finally, since virtualenv looks for pip*.whl in virtualenv_support, this will also work: $ mv /path/to/site-packages/virtualenv_support/pip*.whl{,bak} $ pip wheel -w /path/to/site-packages/virtualenv_support/ 'pip==18' All new virtualenvs will use the version of pip that you installed into virtualenv_support. However, this feels hacky. (Attempted with virtualenv==16. This results in all new virtualenvs with pip==18.)"}
{"text": "Question: How do I tar a directory without retaining the directory structure? I'm working on a backup script and want to tar up a file directory: tar czf ~/backup.tgz /home/username/drupal/sites/default/files This tars it up, but when I untar the resulting file, it includes the full file structure: the files are in home/username/drupal/sites/default/files. Is there a way to exclude the parent directories, so that the resulting tar just knows about the last directory (files)?\nAnswer: cd /home/username/drupal/sites/default/files tar czf ~/backup.tgz *"}
{"text": "Question: GitHub Actions unable to set up Python Virtual Environment I need to setup a virtual environment, and install the requirements for my Flask app. However, an error occurs here: sudo apt install python3-venv sudo python3.8 -m venv venv This is the .yml file for my GitHub Actions. name: TEST on: push: branches: [ master ] pull_request: branches: [ master ] jobs: build: runs-on: ubuntu-latest steps: - name: Setup system group run: | if [ ! $( getent group uni ) ]; then sudo addgroup --system uni; fi - name: Setup system user run: | if [[ $(getent passwd uni) = \"\" ]]; then sudo adduser --no-create-home --force-badname --disabled-login --disabled-password --system uni; fi - name: Add user user to group run: | sudo usermod -g uni uni - name: Setup base directory working-directory: / run: | if [ ! -d ./uni/test/app ]; then sudo mkdir -p ./uni/test/app; fi sudo chown uni:uni -R /uni/test sudo chmod 775 -R /uni/test - name: Setup log directory working-directory: /var/log run: | if [ ! -d ./uni/test ]; then sudo mkdir -p ./uni/test; fi sudo chown uni:uni -R ./uni/test sudo chmod 755 -R ./uni/test - uses: actions/checkout@v2 - name: Set up Python 3.8 uses: actions/setup-python@v2 with: python-version: 3.8 - name: Setup Python virtual environment working-directory: /uni/test/app run: | sudo apt install python3-venv sudo python3.8 -m venv venv - name: Install dependencies working-directory: /uni/test/app/venv run: | source ./bin/activate pip install --upgrade pip pip install wheel if [ -f requirements.txt ]; then pip install -r requirements.txt; fi deactivate What am I doing wrong here? And is there a way I can install python3-venv inside the block below? - uses: actions/checkout@v2 - name: Set up Python 3.8 uses: actions/setup-python@v2 with: python-version: 3.8\nAnswer: I just tested a simple workflow here (same as below) to use the virtual env. It doesn't seem you need the sudo apt install python3-env command when using the actions/checkout and the setup-python actions to create the virtual env using the python3.8 -m venv env command. on: push: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python 3.8 uses: actions/setup-python@v2 with: python-version: 3.8 - name: Run Python commands run: | pip install --upgrade pip python3.8 -m venv env source env/bin/activate echo \"VIRTUAL ENV:\" $VIRTUAL_ENV Returned me VIRTUAL ENV: /home/runner/work/poc-github-actions/poc-github-actions/env on the workflow run logs."}
{"text": "Question: Echo newline in Bash prints literal \\n How do I print a newline? This merely prints \\n: echo -e \"Hello,\\nWorld!\" Output: Hello,\\nWorld!\nAnswer: Use printf instead: printf \"hello\\nworld\\n\" printf behaves more consistently across different environments than echo."}
{"text": "Question: How do I delete a Git branch locally and remotely?\nAnswer: git push -d ; $ git push --delete"}
{"text": "Question: How can I pipe stderr, and not stdout? I have a program that writes information to stdout and stderr, and I need to process the stderr with grep, leaving stdout aside. Using a temporary file, one could do it in two steps: command > /dev/null 2> temp.file grep 'something' temp.file But how can this be achieved without temp files, using one command and pipes?\nAnswer: First redirect stderr to stdout ; then redirect stdout to /dev/null (without changing where stderr is going): command 2>&1 >/dev/null | grep 'something'."}
{"text": "Question: Regex (grep) for multi-line search needed I'm running a grep to find any *.sql file that has the word select followed by the word customerName followed by the word from. This select statement can span many lines and can contain tabs and newlines. I've tried a few variations on the following: $ grep -liIr --include=\"*.sql\" --exclude-dir=\"\\.svn*\" --regexp=\"select[a-zA-Z0- 9+\\n\\r]*customerName[a-zA-Z0-9+\\n\\r]*from\" This, however, just runs forever. Can anyone help me with the correct syntax please?\nAnswer: Without the need to install the grep variant pcregrep, you can do a multiline search with grep. $ grep -Pzo \"(?s)^(\\s*)\\N*main.*?{.*?^\\1}\" *.c Explanation: -P activate perl-regexp for grep (a powerful extension of regular expressions) -z Treat the input as a set of lines, each terminated by a zero byte (the ASCII NUL character) instead of a newline. That is, grep knows where the ends of the lines are, but sees the input as one big line. Beware this also adds a trailing NUL char if used with -o, see comments. -o print only matching. Because we're using -z, the whole file is like a single big line, so if there is a match, the entire file would be printed; this way it won't do that. In regexp: (?s) activate PCRE_DOTALL, which means that . finds any character or newline \\N find anything except newline, even with PCRE_DOTALL activated .*? find . in non-greedy mode, that is, stops as soon as possible. ^ find start of line \\1 backreference to the first group (\\s*). This is a try to find the same indentation of method. As you can imagine, this search prints the main method in a C (*.c) source file."}
{"text": "Question: grep: show lines surrounding each match How do I grep and show the preceding and following 5 lines surrounding each matched line?\nAnswer: For BSD or GNU grep you can use -B num to set how many lines before the match and -A num for the number of lines after the match. grep -B 3 -A 2 foo README.txt If you want the same number of lines before and after you can use -C num. grep -C 3 foo README.txt This will show 3 lines before and 3 lines after."}
{"text": "Question: How to specify the private SSH-key to use when executing shell command on Git?\nAnswer: ssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'"}
{"text": "Question: How to send a compressed archive that contains executables so that Google's attachment filter won't reject it I have a directory that I want to compress to send it by e-mail, I've tried this: tar -cvf filename.tar.gz directory_to_compress/ But when I try to send it by e-mail, Google says: filename.tar.gz contains an executable file. For security reasons, Gmail does not allow you to send this type of file. How to compress a directory into a tar.gz file from command line?\nAnswer: tar -cvzf filename.tar.gz directory_to_compress/ Most tar commands have a z option to create a gziped version. Though seems to me the question is how to circumvent Google. I'm not sure if renaming your output file would fool Google, but you could try. I.e., tar -cvzf filename.bla directory_to_compress/ and then send the filename.bla - contents will would be a zipped tar, so at the other end it could be retrieved as usual."}
{"text": "Question: Can I have multiple subfolders with virtual python environments in VS Code? I have a monorepo structured like this: myRepo/ \u251c\u2500 project_1/ \u2502 \u251c\u2500 .venv/ \u2502 \u251c\u2500 main.py \u251c\u2500 project_2/ \u2502 \u251c\u2500 .venv/ \u2502 \u251c\u2500 main.py \u251c\u2500 .gitignore \u251c\u2500 README.md Can VS Code handle multiple python venvs in subfolders? After some googling I managed a find one solution, but its not very elegant. I created a workspace and added the folders /project_1 and /project_2, that worked and I can easily switch and select Python Interpreter. I also need to modify files in /root from time to time so I added this folder as well. All this makes the Explorer folder structure bigger and more cluttered with duplicates of itself: workspace/ \u251c\u2500\u2500 myRepo/ \u2502 \u251c\u2500\u2500 project_1/ \u2502 \u2502 \u251c\u2500\u2500 .venv/ \u2502 \u2502 \u2514\u2500\u2500 main.py \u2502 \u251c\u2500\u2500 project_2/ \u2502 \u2502 \u251c\u2500\u2500 .venv/ \u2502 \u2502 \u2514\u2500\u2500 main.py \u2502 \u251c\u2500\u2500 .gitignore \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 myRepo/ \u251c\u2500\u2500 project_1/ \u2502 \u251c\u2500\u2500 .venv/ \u2502 \u2514\u2500\u2500 main.py \u2514\u2500\u2500 project_2/ \u251c\u2500\u2500 .venv/ \u2514\u2500\u2500 main.py\nAnswer: VS Code has a list of places, where it looks for virtual environments. Only environments located directly under the workspace are picked up automatically. You can also enter custom paths when running the Python: Select Interpreter command, though. Simply select \"Enter interpreter path...\" and navigate to your venv's /bin/python executable: Once you have used a cutom interpreter path, it is known to VS code and will be directly selectable using the Python: Select Interpreter command."}
{"text": "Question: How to install python3.7 and create a virtualenv with pip on Ubuntu 18.04? I'm trying to set up a standard virtual-environment(venv) with python 3.7 on Ubuntu 18.04, with pip (or some way to install packages in the venv). The standard way to install python3.7 seems to be: % sudo apt install python3.7 python3.7-venv % python3.7 -m venv py37-venv but the second command fails, saying: The virtual environment was not created successfully because ensurepip is not available. On Debian/Ubuntu systems, you need to install the python3-venv package using the following command. apt-get install python3-venv You may need to use sudo with that command. After installing the python3-venv package, recreate your virtual environment. Failing command: ['/py37-venv/bin/python3.7', '-Im', 'ensurepip', '--upgrade', '--default-pip'] This is true; there is no ensurepip nor pip installed with this python. And I did install python3.7-venv already (python3-venv is for python3.6 on Debian/Ubuntu). I gather there has been some discussion about this in the python community because of multiple python versions and/or requiring root access, and alternate ways to install python modules via apt or similar. Creating a venv without pip (--without-pip) succeeds, but then there's no way to install packages in the new venv which seems to largely defeat the purpose. So what's the accepted \"best practice\" way to install and use python3.7 on 18.04 with a venv?\nAnswer: I don't know if it's best practices or not, but if I also install python3-venv and python3.7-venv then everything works (this is tested on a fresh stock Debian buster docker image): % sudo apt install python3.7 python3-venv python3.7-venv % python3.7 -m venv py37-venv % . py37-venv/bin/activate (py37-venv) % Note that it also installs all of python3.6 needlessly, so I can't exactly say I like it, but at least it does work and doesn't require running an unsigned script the way get-pip.py does."}
{"text": "Question: How to reload .bashrc settings without logging out and back in again?\nAnswer: You can enter the long form command: source ~/.bashrc or you can use the shorter version of the command: . ~/.bashrc"}
{"text": "Question: Grep regex NOT containing a string I am passing a list of regex patterns to grep to check against a syslog file. They are usually matching an IP address and log entry; grep \"1\\.2\\.3\\.4.*Has exploded\" syslog.log It's just a list of patterns like the \"1\\.2\\.3\\.4.*Has exploded\" part I am passing, in a loop, so I can't pass \"-v\", for example. I am confused trying to do the inverse of the above, and not match lines with a certain IP address and error so \"!1.2.3.4.*Has exploded\" will match syslog lines for anything other than 1.2.3.4 telling me it has exploded. I must be able to include an IP address to not match. I have seen various similar posts on Stack Overflow. However, they use regex patterns that I can't seem to get to work with grep. What would be a working example for grep? This is happening in a script like this; patterns[1]=\"1\\.2\\.3\\.4.*Has exploded\" patterns[2]=\"5\\.6\\.7\\.8.*Has died\" patterns[3]=\"\\!9\\.10\\.11\\.12.*Has exploded\" for i in {1..3} do grep \"${patterns[$i]}\" logfile.log done\nAnswer: grep matches, grep -v does the inverse. If you need to \"match A but not B\" you usually use pipes: grep \"${PATT}\" file | grep -v \"${NOTPATT}\""}
{"text": "Question: Check if a python module exists with specific venv path If I have a venv path, how can I find out if a Python module is installed inside that venv? I normally use importlib.util.find_spec. But this only works for the current venv that is active and does not work if I have a different venv path. from importlib.util import find_spec if find_spec('numpy'): # Do something\nAnswer: import sys import importlib.util import os from pathlib import Path def is_module_installed_in_venv(module_name, venv_path): venv_python_lib_path = Path(venv_path) / 'lib' for python_dir in venv_python_lib_path.iterdir(): if python_dir.name.startswith('python'): site_packages_path = python_dir / 'site-packages' break if not site_packages_path.exists(): return False sys.path.insert(0, str(site_packages_path)) module_spec = importlib.util.find_spec(module_name) sys.path.pop(0) return module_spec is not None venv_path = '/home/user/anaconda3/envs/env_name/' module_name = 'numpy' if is_module_installed_in_venv(module_name, venv_path): print(\"do something\") this works , make sure to include the full path"}
{"text": "Question: How to avoid \"Permission denied\" when using pip with virtualenv I attempt to deploy a Python package with pip in a virtual environment on an Ubuntu machine, but encounter a permission-related issue. For example: (TestVirtualEnv)test@testServer:~$ pip install markdown2 terminates by: error: could not create '/home/test/virtualenvs/TestVirtualEnv/lib/python3.3/site-packages/markdown2.py': Permission denied I can't sudo, since it will install the package globally, and not within the virtual environment. I chowned site-packages; ls shows only directories related to easy_install, pip and setuptools, and nothing related to Markdown. How to deploy a package in a virtual environment with pip without encountering permission-related errors?\nAnswer: virtualenv permission problems might occur when you create the virtualenv as sudo and then operate without sudo in the virtualenv. As found out in your question's comment, the solution here is to create the virtualenv without sudo to be able to work (esp. write) in it without sudo."}
{"text": "Question: How can I grep for a string that begins with a dash/hyphen? I want to grep for the string that starts with a dash/hyphen, like -X, in a file, but it's confusing this as a command line argument. I've tried: grep \"-X\" grep \\-X grep '-X'\nAnswer: Use: grep -- -X Documentation Related: What does a bare double dash mean? (thanks to nutty about natty)."}
{"text": "Question: echo that outputs to stderr Is there a standard Bash command that acts like echo but outputs to stderr rather than stdout?\nAnswer: You could do this, which facilitates reading: >&2 echo \"error\" >&2 copies file descriptor #2 to file descriptor #1. Therefore, after this redirection is performed, both file descriptors will refer to the same file: the one file descriptor #2 was originally referring to."}
{"text": "Question: How do I prompt for Yes/No/Cancel input in a Linux shell script? I want to pause input in a shell script, and prompt the user for choices. The standard Yes, No, or Cancel type question. How do I accomplish this in a typical bash prompt?\nAnswer: A widely available method to get user input at a shell prompt is the read command. Here is a demonstration: while true; do read -p \"Do you wish to install this program? \" yn case $yn in [Yy]* ) make install; break;; [Nn]* ) exit;; * ) echo \"Please answer yes or no.\";; esac done"}
{"text": "Question: How do I remove a Git submodule?\nAnswer: 1. git submodule deinit -f -- a/submodule 2. rm -rf .git/modules/a/submodule 3. git rm -f a/submodule"}
{"text": "Question: How do I make Git forget about a file that was tracked, but is now in .gitignore?\nAnswer: git rm -r --cached"}
{"text": "Question: How do I remove local (untracked) files from the current Git working tree?\nAnswer: git-clean"}
{"text": "Question: How to install Go in alpine linux I am trying to install Go inside an Alpine Docker image. For that I downloaded tar file from here inside my alpine docker image, untar it using following command: tar -C /usr/local -xzf go1.10.3.linux-amd64.tar.gz exported PATH to have go binary as: export PATH=$PATH:/usr/local/go/bin However, when I say go version then it says that sh: go: not found. I am quite new to alpine. Does anyone know, what I am missing here? Steps to reproduce- $ docker run -it alpine sh $ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz $ tar -C /usr/local -xzf go1.10.3.linux-amd64.tar.gz $ export PATH=$PATH:/usr/local/go/bin $ go version\nAnswer: Thanks BMitch. I compiled the go source code and performed the below steps inside alpine image container. echo \"installing go version 1.10.3...\" apk add --no-cache --virtual .build-deps bash gcc musl-dev openssl go # download go tar wget -O go.tgz https://dl.google.com/go/go1.10.3.src.tar.gz tar -C /usr/local -xzf go.tgz cd /usr/local/go/src/ # compile code ./make.bash export PATH=\"/usr/local/go/bin:$PATH\" export GOPATH=/opt/go/ export PATH=$PATH:$GOPATH/bin apk del .build-deps go version"}
{"text": "Question: How to find patterns across multiple lines using grep? I want to find files that have \"abc\" AND \"efg\" in that order, and those two strings are on different lines in that file. Eg: a file with content: blah blah.. blah blah.. blah abc blah blah blah.. blah blah.. blah blah.. blah efg blah blah blah blah.. blah blah.. Should be matched.\nAnswer: Grep is an awkward tool for this operation. pcregrep which is found in most of the modern Linux systems can be used as pcregrep -M 'abc.*(\\n|.)*efg' test.txt where -M, --multiline allow patterns to match more than one line There is a newer pcre2grep also. Both are provided by the PCRE project. pcre2grep is available for Mac OS X via Mac Ports as part of port pcre2: % sudo port install pcre2 and via Homebrew as: % brew install pcre or for pcre2 % brew install pcre2 pcre2grep is also available on Linux (Ubuntu 18.04+) $ sudo apt install pcre2-utils # PCRE2 $ sudo apt install pcregrep # Older PCRE"}
{"text": "Question: How do I update or sync a forked repository on GitHub?\nAnswer: In your local clone of your forked repository, you can add the original GitHub repository as a \"remote\". Then you can fetch all the branches from that upstream repository, and rebase your work to continue working on the upstream version. In terms of commands that might look like: # Add the remote, call it \"upstream\": git remote add upstream https://github.com/whoever/whatever.git # Fetch all the branches of that remote into remote-tracking branches git fetch upstream # Make sure that you're on your main branch: git checkout main # Rewrite your main branch so that any commits of yours that # aren't already in upstream/main are replayed on top of that # other branch: git rebase upstream/main If you don't want to rewrite the history of your main branch, then you should replace the last command with git merge upstream/main."}
{"text": "Question: How can I find all of the distinct file extensions in a folder hierarchy? On a Linux machine I would like to traverse a folder hierarchy and get a list of all of the distinct file extensions within it. What would be the best way to achieve this from a shell?\nAnswer: Try this (not sure if it's the best way, but it works): find . -type f | perl -ne 'print $1 if m/\\.([^.\\/]+)$/' | sort -u It work as following: Find all files from current folder Prints extension of files if any Make a unique sorted list"}
{"text": "Question: Install python3-venv module on linux mint I was able to move to Linux mint 17.3 64 bit version from my Linux mint 16. This was long awaited migration. After moving to Linux Mint 17.3, I am not able to the install python3-venv module, which is said to be the replacement for virtualenv in python 3.x. In my linux mint 16 I had access to pyvenv-3.4 tool. I dont know when I installed that module in Linux mint 16. Anybody faced this issue ? python -m venv test The virtual environment was not created successfully because ensurepip is not available. On Debian/Ubuntu systems, you need to install the python3-venv package using the following command. apt-get install python3-venv You may need to use sudo with that command. After installing the python3-venv package, recreate your virtual environment. izero@Ganesha ~/devel $ sudo apt-get install python3-venv [sudo] password for izero: Reading package lists... Done Building dependency tree Reading state information... Done E: Unable to locate package python3-venv\nAnswer: Try running this command: sudo apt-get install python3.4-venv Then use this: python3 -m venv test the package name is python3.4-venv and not python3-venv."}
{"text": "Question: How do I get the current branch name in Git?\nAnswer: git rev-parse --abbrev-ref HEAD"}
{"text": "Question: How can I use a postactivate script using Python 3 venv? I'm using venv (used pyvenv to create the environment) and would like to set up environment variables here, but postactivate looks like a virtualenv thing. Can this be done with venv?\nAnswer: venv has the activate script which you can modify to add your environment variables. I would add the variables at the bottom, making a nice comment block to clearly separate the core functionality and my custom variables."}
{"text": "Question: How do I list all the files in a commit? How can I print a plain list of all files that were part of a given commit? Although the following lists the files, it also includes unwanted diff information for each: git show a303aa90779efdd2f6b9d90693e2cbbbe4613c1d\nAnswer: $ git diff-tree --no-commit-id --name-only bd61ad98 -r index.html javascript/application.js javascript/ie6.js"}
{"text": "Question: Make a Bash alias that takes a parameter?\nAnswer: Bash alias does not directly accept parameters. You will have to create a function. alias does not accept parameters but a function can be called just like an alias. For example: myfunction() { #do things with parameters like $1 such as mv \"$1\" \"$1.bak\" cp \"$2\" \"$1\" } myfunction old.conf new.conf #calls `myfunction`"}
{"text": "Question: How can I pass a parameter to Bash function?\nAnswer: To pass parameter in Bash function: function function_name { command... } or function_name () { command... } and To call a function with arguments: function_name \"$arg1\" \"$arg2\""}
{"text": "Question: Pipe to/from the clipboard in a Bash script Is it possible to pipe to/from the clipboard in Bash? Whether it is piping to/from a device handle or using an auxiliary application, I can't find anything. For example, if /dev/clip was a device linking to the clipboard we could do: cat /dev/clip # Dump the contents of the clipboard cat foo > /dev/clip # Dump the contents of \"foo\" into the clipboard\nAnswer: 2018 answer Use clipboard-cli. It works with macOS, Windows, Linux, OpenBSD, FreeBSD, and Android without any real issues. Install it with: npm install -g clipboard-cli Then you can do: echo foo | clipboard If you want, you can alias to cb by putting the following in your .bashrc, .bash_profile, or .zshrc: alias cb=clipboard"}
{"text": "Question: Downgrade Python version in virtual environment I am always getting the same error regarding TensorFlow: ModuleNotFoundError: No module named 'tensorflow.contrib'. I am actually using Python version 3.9 but, reading online, it seems that version 3.7 is the last stable one that can work with TensorFlow version >2.0. Unfortunately I have started my project in a venv with the wrong version of Python and I would like to downgrade it, how can I do that?\nAnswer: Building on @chepner's comment above, since venvs are just directories, you can save your current state and start a fresh virtual environment instead. # Save current installs (venv) -> pip freeze -r > requirements.txt # Shutdown current env (venv) -> deactivate # Copy it to keep a backup -> mv venv venv-3.9 # Ensure you have python3.7 -> python3.7 -V # Create and activate a 3.7 venv -> python3.7 -m venv venv-3.7 -> source venv-3.7/bin/activate # Reinstall previous requirements (venv-3.7) -> pip install -r requirements.txt # Install new requirements Hope that helps!"}
{"text": "Question: How to check out a remote Git branch?\nAnswer: $ git fetch"}
{"text": "Question: Make an existing Git branch track a remote branch?\nAnswer: Given a branch foo and a remote upstream: git branch -u upstream/foo Or, if local branch foo is not the current branch: git branch -u upstream/foo foo"}
{"text": "Question: pylint false positive E0401 import errors in vscode while using venv I created a venv using python3.6 on my mac os in this folder /Users/kim/Documents/Apps/PythonApps/python36-miros-a3 I ran a pip install pylint after I activated the virtual env My workspace is in /Users/kim/Documents/Apps/WebApps/miros-a3 Inside my vscode workspace, I have the following Workspace settings { \"folders\": [ { \"path\": \".\" } ], \"settings\": { \"python.pythonPath\": \"/Users/kim/Documents/Apps/PythonApps/python36-miros-a3/bin/python3.6\", \"python.venvPath\": \"/Users/kim/Documents/Apps/PythonApps\" } } I have tried setting a custom path for the pylint and also changing the venvpath. The pylint kept complaining about the import statement saying it does not exist. As you can see, they are in the same folder, and I can definitely execute my python files. What can I do to avoid these kind of false positive import errors? I have also tried the following: go to commandline turn on the virtual env and then type code to activate the vscode as recommended here https://code.visualstudio.com/docs/setup/mac also tried this https://donjayamanne.github.io/pythonVSCodeDocs/docs/troubleshooting_linting/\nAnswer: Pylint has some quirks. In this case it doesn't know where to find your module because it's in subdirectory of your venv path. To solve this: Put this setting in your workspace or folder settings: \"python.linting.pylintArgs\": [ \"--init-hook\", \"import sys; sys.path.append('')\" ] or, maybe better Generate .pylintrc file. From integrated terminal with venv activated run: pylint --generate-rcfile > .pylintrc then open the generated file and uncomment the init-hook= part to be: init-hook='import sys; sys.path.append(\"\")' Read the .pylintrc and tweak settings if you wish. In both cases path should point to your 'database' folder. After learning about pylint settings, do it the right way: from database.database_dispatcher import ... See this answer by Anthony Sottile."}
